---
title: "Rdimtools MDS size capability"
author: "Adrià Casanova Lloveras"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r libraries, message=F}
library(Rdimtools)
library(dimRed)
library(microbenchmark)
library(mixtools)
library(dplyr)
library(ggplot2)

set.seed(42)

figures_dir <- file.path("figures")
if (!dir.exists(figures_dir)) {
  dir.create(figures_dir)
}

file_name <- "MDS_benchmark_R_lib.csv"
save_benchmark <- function(time, library, dataset, n_obs, d) {
  mbm_df <- as.data.frame(time)
  colnames(mbm_df) <- c("time_ns")
  mbm_df$dataset <- dataset
  mbm_df$library <- library
  mbm_df$n_obs <- n_obs
  mbm_df$d <- d
  if (!file.exists(file_name)) {
    # If it doesn’t exist, write with headers
    write.csv(mbm_df, file_name, row.names = FALSE)
  } else {
    # If it exists, append without writing headers again
    write.table(mbm_df, file_name, row.names = FALSE, col.names = FALSE,
                sep = ",", append = TRUE)
  }
}
```

The goal of this analysis is to test the limits of $\texttt{Rdimtools::do.mds}$. Specifically, we will run Rdimtools's implementation of Classical MDS on datasets of increasing number of observations and dimensionality in order to test its limitations as well as its complexity. We will also compare it with $\texttt{dimRed::embed(.method="MDS", ...)}$.

We will first run the function on the iris dataset to understand it better.

# Performance on the iris dataset

```{r load iris}
# load iris dataset
data(iris)
x <- as.matrix(iris[, 1:4])
lab <- as.factor(iris[, 5])
```

```{r iris run do.mds}
rdimtools_iris <- do.mds(x, ndim = 2)
dimred_iris <- embed(x, "MDS", ndim = 2)
dimred_iris <- as.data.frame(dimred_iris@data@data)
```

```{r iris visualization}
plot(rdimtools_iris$Y, pch = 19, col = lab, main = "Rdimtools on Iris")
plot(dimred_iris, pch = 19, col = lab, main = "dimRed on Iris")
```

```{r iris benchmark}
mbm <- microbenchmark(
  rdimtools_iris = do.mds(x, ndim = 2),
  times = 100 # Number of iterations
)
save_benchmark(mbm$time, "Rdimtools", "iris", n_obs = nrow(iris), d = 4)

mbm <- microbenchmark(
  dimred_iris = embed(x, .method = "MDS", ndim = 2),
  times = 100 # Number of iterations
)
save_benchmark(mbm$time, "dimRed", "iris", n_obs = nrow(iris), d = 4)
```

# Performance on four isotropic Gaussian blobs

```{r gmm test definition}
# Define dataset sizes
n_obs_list <- round(10^seq(log10(1e2), log10(1e4), length.out = 10))
d_list <- c(8, 32, 64) # in ReichmanHagele2024 it is c(64, 512, 2048)
params_grid <- expand.grid(n_obs = n_obs_list, d = d_list)

benchmark_gmm <- function(d, n_obs) {
  # Generate four isotropic Gaussian blobs centered uniformly at random in
  # [-1,1]^d with sd generated uniformly at random in (0,1].
  # Blobs are close to avoid the curse of dimensionality.
  means <- matrix(
    runif(4 * d, min = -1, max = 1),
    nrow = 4, byrow = TRUE
  )
  covariances <- matrix(
    runif(4 * d, min = 0, max = 1),
    nrow = 4, byrow = TRUE
  )
  data <- rmvnormmix(n = n_obs, lambda = rep(1, 4) / 4, mu = means,
                     sigma = covariances)

  # Benchmark Rdimtools::do.mds on the generated dataset
  print(paste0("Benchmarking Rdimtools::do.mds on gmm dataset with d=", d,
               " and n_obs=", n_obs, "..."))
  mbm <- microbenchmark(
    Rdimtools.gmm.random = do.mds(data, ndim = 2),
    times = 20
  )
  save_benchmark(mbm$time, "Rdimtools", "gmm.random", n_obs, d)

  # We will compare Rdimtools and dimRed only for d=8
  if (d > 8) {
    return()
  }

  # Benchmark dimRed::embed(.method="MDS", ...) on the generated dataset
  print(paste0("Benchmarking dimRed::embed(.method='MDS', ...) on gmm dataset with d=", d, " and n_obs=", n_obs, "...")) # nolint
  mbm <- microbenchmark(
    dimRed.gmm.random = embed(data, .method = "MDS", ndim = 2),
    times = 20
  )
  save_benchmark(mbm$time, "dimRed", "gmm.random", n_obs, d)
}
```

```{r gmm test run}
apply(params_grid, 1, function(params) {
                                        benchmark_gmm(params["d"],
                                          params["n_obs"]
                                        )})
```

Now we will plot the obtained data.

```{r plot comparison}
mds_benchmark_data <- read.csv(file_name, header = TRUE,
                               stringsAsFactors = TRUE)
mds_benchmark_data$d <- as.factor(mds_benchmark_data$d)
summary(mds_benchmark_data)

# Summarize data for each library and d combination
summary_data <- mds_benchmark_data %>%
  filter(d == 8) %>%
  group_by(library, n_obs) %>%
  summarize(
    avg_time = mean(time_ns / 1e9),
    sd_time = sd(time_ns / 1e9),
    n_samples = n()
  ) %>%
  ungroup() %>%
  mutate(
    se_time = sd_time / sqrt(n_samples),
    upper_time = avg_time + se_time,
    lower_time = avg_time - se_time
  )

# Plot
ggplot(
  summary_data,
  aes(x = n_obs, y = avg_time, color = library)
) +
  geom_line() +
  geom_errorbar(aes(ymin = lower_time, ymax = upper_time),
    width = 0.02
  ) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "MDS in R benchmark",
    subtitle = "Average Runtime by n_obs (d = 8)",
    x = "n_obs",
    y = "Time (s)"
  ) +
  theme_minimal()

# Save the plot as a PDF file
ggsave(file.path(figures_dir, "MDS_dimRed_vs_Rdimtools.pdf"),
  width = 8, height = 6, dpi = 300
)
```

```{r plot Rdimtools boxplots}
# Summarize the data to calculate average and standard error of time for each
# n_obs and d
summary_data <- mds_benchmark_data %>%
  filter(library == "Rdimtools") %>%
  group_by(n_obs, d) %>%
  summarize(
    avg_time = mean(time_ns / 1e9),
    sd_time = sd(time_ns / 1e9),
    n_samples = n()
  ) %>%
  ungroup() %>%
  mutate(
    se_time = sd_time / sqrt(n_samples),
    upper_time = avg_time + se_time,
    lower_time = avg_time - se_time
  )

# Plot
ggplot(
  summary_data,
  aes(x = n_obs, y = avg_time, color = d)
) +
  geom_line() +
  geom_errorbar(aes(ymin = lower_time, ymax = upper_time),
    width = 0.02
  ) +
  scale_x_log10() +
  scale_y_log10() +
  labs(
    title = "MDS in R benchmark",
    subtitle = "Average Runtime by n_obs (library = Rdimtools)",
    x = "n_obs",
    y = "Time (s)"
  ) +
  theme_minimal()

# Save plot
ggsave(file.path(figures_dir, "MDS_Rdimtools.pdf"),
  width = 8, height = 6, dpi = 300
)
```

According to the previous plot, Rdimtools can handle larger datasets, so we will test it further.

```{r gmm test large n_obs}
# Define dataset sizes to expand previous test
n_obs_list <- round(10^seq(log10(1e4), log10(1e5), length.out = 5))
n_obs_list <- n_obs_list[-1]
d_list <- c(8, 32, 64) # in ReichmanHagele2024 it is c(64, 512, 2048)
params_grid <- expand.grid(n_obs = n_obs_list, d = d_list)

benchmark_gmm <- function(d, n_obs) {
  # Skip already tested scenarios
  if (d == 8 || n_obs > 50000) {
    return()
  }
  # Generate four isotropic Gaussian blobs centered uniformly at random in
  # [-1,1]^d with sd generated uniformly at random in (0,1].
  # Blobs are close to avoid the curse of dimensionality.
  means <- matrix(
    runif(4 * d, min = -1, max = 1),
    nrow = 4, byrow = TRUE
  )
  covariances <- matrix(
    runif(4 * d, min = 0, max = 1),
    nrow = 4, byrow = TRUE
  )
  data <- rmvnormmix(n = n_obs, lambda = rep(1, 4) / 4, mu = means,
                     sigma = covariances)

  # Benchmark Rdimtools::do.mds on the generated dataset
  print(paste0("Benchmarking Rdimtools::do.mds on gmm dataset with d=", d,
               " and n_obs=", n_obs, "..."))
  mbm <- microbenchmark(
    Rdimtools.gmm.random = do.mds(data, ndim = 2),
    times = 20
  )
  save_benchmark(mbm$time, "Rdimtools", "gmm.random", n_obs, d)
}
apply(params_grid, 1, function(params) {
                                        benchmark_gmm(params["d"],
                                                      params["n_obs"])})
```

R crashes for when n_obs == 56234 for all dimensions tested. This is probably caused by the lack of necessary RAM.

We can observe as well quadratic time complexity in Rdimtools' implementation, although dimRed is slower in general and shows a larger complexity.

Regarding dimensionality, it does not have a significant impact in the maximum dataset size MDS can handle. In other words, the same limitation in number of rows was reached in all dimensionalities tested (8, 32 and 64). Moreover, it has linear time complexity , although we cannot be absolutely sure because only three dimensionalities were tested.
