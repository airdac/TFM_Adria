@article{
	paradis2021,
	author = {Paradis, Emmanuel},
	year = {2021},
	volume = {},
	pages = {Published on-line 05 June 2021},
	title = {Reduced multidimensional scaling},
	journal = {Computational Statistics},
	doi = {10.1007/s00180-021-01116-0}
}

@inproceedings{
	metric_map,
	author = {Wang, Jason Tsong-Li and Wang, Xiong and Lin, King-Ip and Shasha, Dennis and Shapiro, Bruce A. and Zhang, Kaizhong},
	title = {Evaluating a Class of Distance-Mapping Algorithms for Data Mining and Clustering},
	year = {1999},
	isbn = {1581131437},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/312129.312264},
	doi = {10.1145/312129.312264},
	booktitle = {Proceedings of the Fifth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
	pages = {307–311},
	numpages = {5},
	location = {San Diego, California, USA},
	series = {KDD '99}
}

@inproceedings{
	Yang06afast,
	author = {Tynia Yang and Jinze Liu and Leonard McMillan and Wei Wang},
	title = {A fast approximation to multidimensional scaling},
	booktitle = {Proceedings of the ECCV Workshop on Computation Intensive Methods for Computer Vision (CIMCV)},
	year = {2006}
}

@book{
	BorgGroenen2005,
	author = {Borg, I. and Groenen, P.},
	biburl = {https://www.bibsonomy.org/bibtex/2b9360235f054e3b2ff0e4b4a0c63b4eb/tmalsburg},
	publisher = {Springer},
	title = {Modern Multidimensional Scaling: Theory and Applications},
	year = {2005}
}

@book{
	GowerHand:1995,
	author = {Gower, John C and Hand, David J},
	publisher = {CRC Press},
	title = {Biplots},
	volume = {54},
	year = {1995}
}

@book{
	krzanowski2000principles,
	 author = {Krzanowski, Wojtek},
	 edition = {Revised},
	 publisher = {OUP Oxford},
	 series = {Oxford Statistical Science Series},
	 subtitle = {A User's Perspective},
	 title = {Principles of Multivariate Analysis},
	 volume = {23},
	 year = {2000}
}

@book{
	johnson2002applied,
	author = {Johnson, Richard Arnold and Wichern, Dean W.},
	edition = {5th},
	publisher = {Prentice Hall},
	title = {Applied Multivariate Statistical Analysis},
	year = {2002}
}

@book{
	trefethen97,
	added-at = {2010-09-19T02:35:23.000+0200},
	author = {Trefethen, Lloyd N. and Bau, David},
	biburl = {https://www.bibsonomy.org/bibtex/2e45a2ed5ccc6dc12721cde613217c222/ytyoun},
	interhash = {1e7e7a44cbff3092be50a71fe056c8ec},
	intrahash = {e45a2ed5ccc6dc12721cde613217c222},
	isbn = {0898713617},
	keywords = {characteristic eigenvalues linear.algebra matrix numerical numerical.analysis polynomial secular.equation textbook},
	publisher={Society for Industrial and Applied Mathematics},
	timestamp = {2017-11-25T07:18:16.000+0100},
	title = {Numerical Linear Algebra},
	year = {1997}
}

@manual{
	pdistPackage,
	author = {Jeffrey Wong},
	note = {R package version 1.2},
	title = {pdist: Partitioned Distance Function},
    url = {https://CRAN.R-project.org/package=pdist},
    year = {2013}
}

@manual{
	Rprogram,
	address = {Vienna, Austria},
	author = {{R Core Team}},
	organization = {R Foundation for Statistical Computing},
	title = {R: A Language and Environment for Statistical Computing},
	url = {https://www.R-project.org/},
	year = {2020}
}

@INPROCEEDINGS{
	emnist,
	author={Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and van Schaik, André},
	booktitle={2017 International Joint Conference on Neural Networks (IJCNN)}, 
	title={EMNIST: Extending MNIST to handwritten letters}, 
	year={2017},
	volume={},
	number={},
	pages={2921-2926},
	doi={10.1109/IJCNN.2017.7966217}}

@misc{
	nist_special,
	author = {Patrick Grother},
	title = {{NIST} Special Database 19. {NIST} Handprinted Forms and Characters Database},
	year = {1970},
	publisher = {World Wide Web-Internet and Web Information Systems},
	language = {en},
}

@misc{
	mnist,
	added-at = {2010-06-28T21:16:30.000+0200},
	author = {LeCun, Yann and Cortes, Corinna},
	biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	groups = {public},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	interhash = {21b9d0558bd66279df9452562df6e6f3},
	intrahash = {935bad99fa1f65e03c25b315aa3c1032},
	keywords = {MSc _checked character_recognition mnist network neural},
	lastchecked = {2016-01-14 14:24:11},
	timestamp = {2016-07-12T19:25:30.000+0200},
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	username = {mhwombat},
	year = 2010
}

@article{
	bentley1980general,
	title = {A general method for solving divide-and-conquer recurrences},
	author ={Bentley, Jon Louis and Haken, Dorothea and Saxe, James B},
	journal = {ACM SIGACT News},
	volume = {12},
	number = {3},
	pages = {36--44},
	year = {1980},
	publisher = {ACM New York, NY, USA}
}

@article{
	kristof,
	title = {A theorem on the trace of certain matrix products and some applications},
	journal = {Journal of Mathematical Psychology},
	volume = {7},
	number = {3},
	pages = {515-530},
	year = {1970},
	issn = {0022-2496},
	doi = {https://doi.org/10.1016/0022-2496(70)90037-4},
	url = {https://www.sciencedirect.com/science/article/pii/0022249670900374},
	author = {Walter Kristof},
	abstract = {A theorem giving attainable upper and lower limits for the trace of certain products of real matrices is established. These products are of the form X1Γ1X2Γ2 … XnΓn with orthogonal matrices Xi and diagonal matrices Γi where the matrices Xi are allowed to vary independently and unrestrictedly. The proof makes use of two lemmas. The theorem may find application in psychometrics when the trace of matrices is involved. Several examples taken from this area are appended by way of illustration.}
}

@misc{
	Pachon_Garcia_2019, 
	author={Pach\'on-Garc\'{\i}a, Cristian}, 
	title={Multidimensional scaling for Big Data}, 
    year={2019},
	howpublished ={http://hdl.handle.net/2117/127318}, 
	organization={UPC, Facultat de Matem\`atiques i Estad\'{\i}stica, Master in Statistics and Operations Research}
}

@article{
	Gower:1968,
	author={Gower, John C},
	title={Adding a point to vector diagrams in multivariate analysis},
	year={1968},
	journal= {Biometrika},
	volume= 55,
	number= 3,
	pages={582-–585}
}

@article{
	Paradis:2018,
	author = {Emmanuel Paradis},
	title = {Multidimensional Scaling With Very Large Datasets},
	journal = {Journal of Computational and Graphical Statistics},
	volume = {27},
	number = {4},
	pages = {935--939},
	year  = {2018},
	publisher = {Taylor & Francis},
	doi = {10.1080/10618600.2018.1470001},
	URL = {https://doi.org/10.1080/10618600.2018.1470001},
	eprint = {https://doi.org/10.1080/10618600.2018.1470001}
}

@article{
	Mardia:1978,
	author = {   K.V.   Mardia },
	title = {Some properties of clasical multi-dimesional scaling},
	journal = {Communications in Statistics - Theory and Methods},
	volume = {7},
	number = {13},
	pages = {1233-1241},
	year  = {1978},
	publisher = {Taylor & Francis},
	doi = {10.1080/03610927808827707},
	URL = {https://doi.org/10.1080/03610927808827707},
	eprint = {https://doi.org/10.1080/03610927808827707}
}

@book{
	MardiaKentBibby:1979,
	title={Multivariate analysis},
	author={Mardia, KV and Kent, M and Bibby, JM },
	series={Probability and Mathematical Statistics}, 
	publisher={Academic Press},
	year={1979}
}

@Manual{
	svd_package:2022,
	title = {svd: Interfaces to Various State-of-Art SVD and Eigensolvers},
	author = {Anton Korobeynikov and Rasmus Munk Larsen and Lawrence Berkeley National Laboratory},
	year = {2022},
	note = {R package version 0.5.2},
	url = {https://CRAN.R-project.org/package=svd},
}

@Article{
	RcppEigen:2013,
	title = {Fast and Elegant Numerical Linear Algebra Using the {RcppEigen} Package},
	author = {Douglas Bates and Dirk Eddelbuettel},
	journal = {Journal of Statistical Software},
	year = {2013},
	volume = {52},
	number = {5},
	pages = {1--24},
	doi = {10.18637/jss.v052.i05},
}

@techreport{
	LMDS:2004,
	title={Sparse multidimensional scaling using landmark points},
	author={De Silva, Vin and Tenenbaum, Joshua B},
	year={2004},
	institution={Technical Report, Stanford University}
}

@article{
	LMDS_ensamble:2009,
	title={Landmark MDS ensemble},
	author={Lee, Seunghak and Choi, Seungjin},
	journal={Pattern recognition},
	volume={42},
	number={9},
	pages={2045--2053},
	year={2009},
	publisher={Elsevier}
}

@article{
	torgerson1952multidimensional,
	title={Multidimensional scaling: I. Theory and method},
	author={Torgerson, Warren S},
	journal={Psychometrika},
	volume={17},
	number={4},
	pages={401--419},
	year={1952},
	publisher={Springer}
}

@article{
	gower1966some,
	title={Some distance properties of latent root and vector methods used in multivariate analysis},
	author={Gower, John C},
	journal={Biometrika},
	volume={53},
	number={3-4},
	pages={325--338},
	year={1966},
	publisher={Oxford University Press}
}

@article{
	saeed2018survey,
	title={A survey on multidimensional scaling},
	author={Saeed, Nasir and Nam, Haewoon and Haq, Mian Imtiaz Ul and Muhammad Saqib, Dost Bhatti},
	journal={ACM Computing Surveys (CSUR)},
	volume={51},
	number={3},
	pages={1--25},
	year={2018},
	publisher={ACM New York, NY, USA}
}

@article{
	Shepard:1962:MDS_1,
	title={The analysis of proximities: {M}ultidimensional scaling with an unknown distance function. I.},
	author={Shepard, Roger N},
	journal={Psychometrika},
	volume={27},
	number={2},
	pages={125--140},
	year={1962},
	publisher={Springer}
}

@article{
	Shepard:1962:MDS_2,
	title={The analysis of proximities: {M}ultidimensional scaling with an unknown distance function. II},
	author={Shepard, Roger N},
	journal={Psychometrika},
	volume={27},
	number={3},
	pages={219--246},
	year={1962},
	publisher={Springer}
}

@article{
	Kruskal:1964:a,
	title={Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis},
	author={Kruskal, Joseph B},
	journal={Psychometrika},
	volume={29},
	number={1},
	pages={1--27},
	year={1964},
	publisher={Springer}
}

@article{
	Kruskal:1964:b,
	title={Nonmetric multidimensional scaling: {A} numerical method},
	author={Kruskal, Joseph B},
	journal={Psychometrika},
	volume={29},
	number={2},
	pages={115--129},
	year={1964},
	publisher={Springer}
}

@InProceedings{
	Platt:2005,
	title = 	 {FastMap, MetricMap, and Landmark MDS are all Nystr\"om Algorithms},
	author =       {Platt, John},
	booktitle = 	 {Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics},
	pages = 	 {261--268},
	year = 	 {2005},
	editor = 	 {Cowell, Robert G. and Ghahramani, Zoubin},
	volume = 	 {R5},
	series = 	 {Proceedings of Machine Learning Research},
	month = 	 {06--08 Jan},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/r5/platt05a/platt05a.pdf},
	url = 	 {https://proceedings.mlr.press/r5/platt05a.html},
	note =         {Reissued by PMLR on 30 March 2021.}
}


@inproceedings{
	fastmap,
	author = {Faloutsos, Christos and Lin, King-Ip},
	title = {FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets},
	year = {1995},
	isbn = {0897917316},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	url = {https://doi.org/10.1145/223784.223812},
	doi = {10.1145/223784.223812},
	abstract = {A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the 'Query By Example' type (which translates to a range query); the 'all pairs' query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc.However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points.This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient retrieval, in conjunction with a SAM, as discussed before and (b) visualization and data-mining: the objects can now be plotted as points in 2-d or 3-d space, revealing potential clusters, correlations among attributes and other regularities that data-mining is looking for.We introduce an older method from pattern recognition, namely, Multi-Dimensional Scaling (MDS) [51]; although unsuitable for indexing, we use it as yardstick for our method. Then, we propose a much faster algorithm to solve the problem in hand, while in addition it allows for indexing. Experiments on real and synthetic data indeed show that the proposed algorithm is significantly faster than MDS, (being linear, as opposed to quadratic, on the database size N), while it manages to preserve distances and the overall structure of the data-set.},
	booktitle = {Proceedings of the 1995 ACM SIGMOD International Conference on Management of Data},
	pages = {163–174},
	numpages = {12},
	location = {San Jose, California, USA},
	series = {SIGMOD '95}
}

@InProceedings{
	pivot_MDS:2007,
	author="Brandes, Ulrik
	and Pich, Christian",
	editor="Kaufmann, Michael
	and Wagner, Dorothea",
	title="Eigensolver Methods for Progressive Multidimensional Scaling of Large Data",
	booktitle="Graph Drawing",
	year="2007",
	publisher="Springer Berlin Heidelberg",
	address="Berlin, Heidelberg",
	pages="42--53",
	abstract="We present a novel sampling-based approximation technique for classical multidimensional scaling that yields an extremely fast layout algorithm suitable even for very large graphs. It produces layouts that compare favorably with other methods for drawing large graphs, and it is among the fastest methods available. In addition, our approach allows for progressive computation, i.e. a rough approximation of the layout can be produced even faster, and then be refined until satisfaction.",
	isbn="978-3-540-70904-6"
}

@Article{
	pivot_mds_R,
	author = {{David Schoch}},
	title = {graphlayouts: Layout algorithms for network visualizations
	in R},
	year = {2023},
	publisher = {The Open Journal},
	volume = {8},
	number = {84},
	pages = {5238},
	journal = {Journal of Open Source Software},
	doi = {10.21105/joss.05238},
	url = {https://doi.org/10.21105/joss.05238},
}

@Manual{
	microbenchmark,    
	title = {microbenchmark: Accurate Timing Functions}, 
	author = {Olaf Mersmann}, 
	year = {2023}, 
	note = {R package version 1.4.10},
	url = {https://CRAN.R-project.org/package=microbenchmark}
}

@Article{
	Rdimtools,
	title = {{Rdimtools}: An {R} Package for Dimension Reduction and Intrinsic Dimension Estimation},
	author = {Kisung You and Dennis Shung},
	journal = {Software Impacts},
	year = {2022},
	volume = {14},
	issn = {26659638},
	pages = {100414},
	doi = {10.1016/j.simpa.2022.100414}
}

@article{
	Lanczos_restart,
	author = {Wu, Kesheng and Simon, Horst},
	title = {Thick-Restart Lanczos Method for Large Symmetric Eigenvalue Problems},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	volume = {22},
	number = {2},
	pages = {602-616},
	year = {2000},
	doi = {10.1137/S0895479898334605},
	URL = { https://doi.org/10.1137/S0895479898334605},
	eprint = {https://doi.org/10.1137/S0895479898334605},
	abstract = { In this paper, we propose a restarted variant of the Lanczos method for symmetric eigenvalue problems named the thick-restart Lanczos method. This new variant is able to retain an arbitrary number of Ritz vectors from the previous iterations with a minimal restarting cost. Since it restarts with Ritz vectors, it is simpler than similar methods, such as the implicitly restarted Lanczos method. We carefully examine the effects of the floating-point round-off errors on stability of the new algorithm and present an implementation of the partial reorthogonalization scheme that guarantees accurate Ritz values with a minimal amount of reorthogonalization. We also show a number of heuristics on deciding which Ritz pairs to save during restart in order to maximize the overall performance of the thick-restart Lanczos method. }
}

@article{
	Lanczos_adaptive,
	author = {Yamazaki, Ichitaro and Bai, Zhaojun and Simon, Horst and Wang, Lin-Wang and Wu, Kesheng},
	title = {Adaptive Projection Subspace Dimension for the Thick-Restart Lanczos Method},
	year = {2010},
	issue_date = {September 2010},
	publisher = {Association for Computing Machinery},
	address = {New York, NY, USA},
	volume = {37},
	number = {3},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/1824801.1824805},
	doi = {10.1145/1824801.1824805},
	abstract = {The Thick-Restart Lanczos (TRLan) method is an effective method for solving large-scale Hermitian eigenvalue problems. The performance of the method strongly depends on the dimension of the projection subspace used at each restart. In this article, we propose an objective function to quantify the effectiveness of the selection of subspace dimension, and then introduce an adaptive scheme to dynamically select the dimension to optimize the performance. We have developed an open-source software package a--TRLan to include this adaptive scheme in the TRLan method. When applied to calculate the electronic structure of quantum dots, a--TRLan runs up to 2.3x faster than a state-of-the-art preconditioned conjugate gradient eigensolver.},
	journal = {ACM Trans. Math. Softw.},
	month = {sep},
	articleno = {27},
	numpages = {18},
	keywords = {electronic structure calculation, thick-restart, Lanczos, Adaptive subspace dimension}
}

@inproceedings{deSilvaTenenbaum2002,
	author = {Silva, Vin and Tenenbaum, Joshua},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {S. Becker and S. Thrun and K. Obermayer},
	pages = {},
	publisher = {MIT Press},
	title = {Global Versus Local Methods in Nonlinear Dimensionality Reduction},
	url = {https://proceedings.neurips.cc/paper_files/paper/2002/file/5d6646aad9bcc0be55b2c82f69750387-Paper.pdf},
	volume = {15},
	year = {2002}
}

@article{Kraemer2018dimRedAC,
  title={dimRed and coRanking - Unifying Dimensionality Reduction in R},
  author={Guido Kraemer and Markus Reichstein and Miguel D. Mahecha},
  journal={R J.},
  year={2018},
  volume={10},
  pages={342},
  url={https://api.semanticscholar.org/CorpusID:62831555}
}

@inproceedings{SalahHenouda2020,
author = {Salah Eddine, Henouda and Kazar, Okba and fatima, laalam and Merizig, Abdelhak},
year = {2020},
month = {11},
pages = {},
title = {Comparative study for dimensionality reduction techniques for big data}
}

@misc{reichmann2024outofcoredimensionalityreductionlarge,
      title={Out-of-Core Dimensionality Reduction for Large Data via Out-of-Sample Extensions}, 
      author={Luca Reichmann and David Hägele and Daniel Weiskopf},
      year={2024},
      eprint={2408.04129},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.04129}, 
}

@article{pyseer,
    author = {Lees, John A and Galardini, Marco and Bentley, Stephen D and Weiser, Jeffrey N and Corander, Jukka},
    title = {pyseer: a comprehensive tool for microbial pangenome-wide association studies},
    journal = {Bioinformatics},
    volume = {34},
    number = {24},
    pages = {4310-4312},
    year = {2018},
    month = {07},
    abstract = {Genome-wide association studies (GWAS) in microbes have different challenges to GWAS in eukaryotes. These have been addressed by a number of different methods. pyseer brings these techniques together in one package tailored to microbial GWAS, allows greater flexibility of the input data used, and adds new methods to interpret the association results.pyseer is written in python and is freely available at https://github.com/mgalardini/pyseer, or can be installed through pip. Documentation and a tutorial are available at http://pyseer.readthedocs.io.Supplementary data are available at Bioinformatics online.},
    issn = {1367-4803},
    doi = {10.1093/bioinformatics/bty539},
    url = {https://doi.org/10.1093/bioinformatics/bty539},
    eprint = {https://academic.oup.com/bioinformatics/article-pdf/34/24/4310/48919461/bioinformatics\_34\_24\_4310.pdf},
}

@article{pedregosa2011scikit,
  title={Scikit-learn: Machine learning in Python},
  author={Pedregosa, Fabian and Varoquaux, Ga{\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},
  journal={Journal of machine learning research},
  volume={12},
  number={Oct},
  pages={2825--2830},
  year={2011}
}

@article{2018arXivUMAP,
     author = {{McInnes}, L. and {Healy}, J. and {Melville}, J.},
     title = "{UMAP: Uniform Manifold Approximation
     and Projection for Dimension Reduction}",
     journal = {ArXiv e-prints},
     archivePrefix = "arXiv",
     eprint = {1802.03426},
     primaryClass = "stat.ML",
     keywords = {Statistics - Machine Learning,
                 Computer Science - Computational Geometry,
                 Computer Science - Learning},
     year = 2018,
     month = feb,
}

@misc{Ulyanov2016,
  author = {Ulyanov, Dmitry},
  title = {Multicore-TSNE},
  year = {2016},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/DmitryUlyanov/Multicore-TSNE}},
}

@article{Policar2024,
    title={openTSNE: A Modular Python Library for t-SNE Dimensionality Reduction and Embedding},
    author={Poli{\v c}ar, Pavlin G. and Stra{\v z}ar, Martin and Zupan, Bla{\v z}},
    journal={Journal of Statistical Software},
    year={2024},
    volume={109},
    number={3},
    pages={1–30},
    doi={10.18637/jss.v109.i03},
    url={https://www.jstatsoft.org/index.php/jss/article/view/v109i03}
}

@article{JMLR:v15:vandermaaten14a,
  author  = {Laurens van der Maaten},
  title   = {Accelerating t-SNE using Tree-Based Algorithms},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {93},
  pages   = {3221--3245},
  url     = {http://jmlr.org/papers/v15/vandermaaten14a.html}
}


@article{fitsne,
	abstract = {t-distributed stochastic neighbor embedding (t-SNE) is widely used for visualizing single-cell RNA-sequencing (scRNA-seq) data, but it scales poorly to large datasets. We dramatically accelerate t-SNE, obviating the need for data downsampling, and hence allowing visualization of rare cell populations. Furthermore, we implement a heatmap-style visualization for scRNA-seq based on one-dimensional t-SNE for simultaneously visualizing the expression patterns of thousands of genes. Software is available at https://github.com/KlugerLab/FIt-SNEand https://github.com/KlugerLab/t-SNE-Heatmaps.},
	author = {Linderman, George C. and Rachh, Manas and Hoskins, Jeremy G. and Steinerberger, Stefan and Kluger, Yuval},
	date = {2019-03-01},
	date-added = {2025-02-17 18:04:41 +0100},
	date-modified = {2025-02-17 18:04:41 +0100},
	doi = {10.1038/s41592-018-0308-4},
	id = {Linderman2019},
	isbn = {1548-7105},
	journal = {Nature Methods},
	number = {3},
	pages = {243--245},
	title = {Fast interpolation-based t-SNE for improved visualization of single-cell RNA-seq data},
	url = {https://doi.org/10.1038/s41592-018-0308-4},
	volume = {16},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41592-018-0308-4}}

@article{direpack,
title = {direpack: A Python 3 package for state-of-the-art statistical dimensionality reduction methods},
journal = {SoftwareX},
volume = {21},
pages = {101282},
year = {2023},
issn = {2352-7110},
doi = {https://doi.org/10.1016/j.softx.2022.101282},
url = {https://www.sciencedirect.com/science/article/pii/S235271102200200X},
author = {Emmanuel Jordy Menvouta and Sven Serneels and Tim Verdonck},
keywords = {Dimensionality reduction, Projection pursuit, Sufficient dimension reduction, Robust statistics, Energy statistics, Statistical learning},
abstract = {The direpack package establishes a set of modern statistical dimensionality reduction techniques into the Python universe as a single, consistent package. Several of the methods included are only available as open source through direpack, whereas the package also offers competitive Python implementations of methods previously only available in other programming languages. In its present version, the package is structured in three subpackages for different approaches to dimensionality reduction: projection pursuit, sufficient dimension reduction and robust M estimators. As a corollary, the package also provides access to regularized regression estimators based on these reduced dimension spaces, as well as a set of classical and robust preprocessing utilities, including very recent developments such as generalized spatial signs. Finally, direpack has been written to be consistent with the scikit-learn API, such that the estimators can flawlessly be included into (statistical and/or machine) learning pipelines in that framework.}
}


@article{phate,
	abstract = {The high-dimensional data created by high-throughput technologies require visualization tools that reveal data structure and patterns in an intuitive form. We present PHATE, a visualization method that captures both local and global nonlinear structure using an information-geometric distance between data points. We compare PHATE to other tools on a variety of artificial and biological datasets, and find that it consistently preserves a range of patterns in data, including continual progressions, branches and clusters, better than other tools. We define a manifold preservation metric, which we call denoised embedding manifold preservation (DEMaP), and show that PHATE produces lower-dimensional embeddings that are quantitatively better denoised as compared to existing visualization methods. An analysis of a newly generated single-cell RNA sequencing dataset on human germ-layer differentiation demonstrates how PHATE reveals unique biological insight into the main developmental branches, including identification of three previously undescribed subpopulations. We also show that PHATE is applicable to a wide variety of data types, including mass cytometry, single-cell RNA sequencing, Hi-C and gut microbiome data.},
	author = {Moon, Kevin R. and van Dijk, David and Wang, Zheng and Gigante, Scott and Burkhardt, Daniel B. and Chen, William S. and Yim, Kristina and Elzen, Antonia van den and Hirn, Matthew J. and Coifman, Ronald R. and Ivanova, Natalia B. and Wolf, Guy and Krishnaswamy, Smita},
	date = {2019-12-01},
	date-added = {2025-02-17 18:43:27 +0100},
	date-modified = {2025-02-17 18:43:27 +0100},
	doi = {10.1038/s41587-019-0336-3},
	id = {Moon2019},
	isbn = {1546-1696},
	journal = {Nature Biotechnology},
	number = {12},
	pages = {1482--1492},
	title = {Visualizing structure and transitions in high-dimensional biological data},
	url = {https://doi.org/10.1038/s41587-019-0336-3},
	volume = {37},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1038/s41587-019-0336-3}}

@inproceedings{rehurek_lrec,
      title = {{Software Framework for Topic Modelling with Large Corpora}},
      author = {Radim {\v R}eh{\r u}{\v r}ek and Petr Sojka},
      booktitle = {{Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}},
      pages = {45--50},
      year = 2010,
      month = May,
      day = 22,
      publisher = {ELRA},
      address = {Valletta, Malta},
      language={English}
}

@article{ParaDime,
author = {Hinterreiter, Andreas and Humer, Christina and Kainz, Bernhard and Streit, Marc},
title = {ParaDime: A Framework for Parametric Dimensionality Reduction},
journal = {Computer Graphics Forum},
volume = {42},
number = {3},
pages = {337-348},
keywords = {CCS Concepts, • Computing methodologies → Neural networks, Learning latent representations, • Human-centered computing → Visualization systems and tools, Information visualization},
doi = {https://doi.org/10.1111/cgf.14834},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14834},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14834},
abstract = {Abstract ParaDime is a framework for parametric dimensionality reduction (DR). In parametric DR, neural networks are trained to embed high-dimensional data items in a low-dimensional space while minimizing an objective function. ParaDime builds on the idea that the objective functions of several modern DR techniques result from transformed inter-item relationships. It provides a common interface for specifying these relations and transformations and for defining how they are used within the losses that govern the training process. Through this interface, ParaDime unifies parametric versions of DR techniques such as metric MDS, t-SNE, and UMAP. It allows users to fully customize all aspects of the DR process. We show how this ease of customization makes ParaDime suitable for experimenting with interesting techniques such as hybrid classification/embedding models and supervised DR. This way, ParaDime opens up new possibilities for visualizing high-dimensional data.},
year = {2023}
}

@inproceedings{Bengio2003OOS,
 author = {Bengio, Yoshua and Paiement, Jean-fran\c{c}cois and Vincent, Pascal and Delalleau, Olivier and Roux, Nicolas and Ouimet, Marie},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Thrun and L. Saul and B. Sch\"{o}lkopf},
 pages = {},
 publisher = {MIT Press},
 title = {Out-of-Sample Extensions for LLE, Isomap, MDS, Eigenmaps, and Spectral Clustering},
 url = {https://proceedings.neurips.cc/paper_files/paper/2003/file/cf05968255451bdefe3c5bc64d550517-Paper.pdf},
 volume = {16},
 year = {2003}
}

@inproceedings{Gisbrecht2012OutofsampleKE,
  title={Out-of-sample kernel extensions for nonparametric dimensionality reduction},
  author={Andrej Gisbrecht and Wouter Lueks and Bassam Mokbel and Barbara Hammer},
  booktitle={The European Symposium on Artificial Neural Networks},
  year={2012},
  url={https://api.semanticscholar.org/CorpusID:15613611}
}

@article{Zhang2021OOS,
author = {Haili Zhang and Pu Wang and Xuejin Gao and Yongsheng Qi and Huihui Gao},
title ={Out-of-sample data visualization using bi-kernel t-SNE},

journal = {Information Visualization},
volume = {20},
number = {1},
pages = {20-34},
year = {2021},
doi = {10.1177/1473871620978209},

URL = { 
    
        https://doi.org/10.1177/1473871620978209
    
    

},
eprint = { 
    
        https://doi.org/10.1177/1473871620978209
    
    

}
,
    abstract = { T-distributed stochastic neighbor embedding (t-SNE) is an effective visualization method. However, it is non-parametric and cannot be applied to steaming data or online scenarios. Although kernel t-SNE provides an explicit projection from a high-dimensional data space to a low-dimensional feature space, some outliers are not well projected. In this paper, bi-kernel t-SNE is proposed for out-of-sample data visualization. Gaussian kernel matrices of the input and feature spaces are used to approximate the explicit projection. Then principal component analysis is applied to reduce the dimensionality of the feature kernel matrix. Thus, the difference between inliers and outliers is revealed. And any new sample can be well mapped. The performance of the proposed method for out-of-sample projection is tested on several benchmark datasets by comparing it with other state-of-the-art algorithms. }
}

@inproceedings{lam2015numba,
  title={Numba: A llvm-based python jit compiler},
  author={Lam, Siu Kwan and Pitrou, Antoine and Seibert, Stanley},
  booktitle={Proceedings of the Second Workshop on the LLVM Compiler Infrastructure in HPC},
  pages={1--6},
  year={2015}
}