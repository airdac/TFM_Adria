\documentclass[a4paper,12pt]{article}
\usepackage[left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm]{geometry}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage[backend=biber]{biblatex}
\addbibresource{../bibliography.bib}
\usepackage{minted}
\setminted{frame=lines, fontsize=\small}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=black,
    urlcolor=black,
    citecolor=black
}
% Allow more flexible breaking of long URLs:
\setlength\emergencystretch{3em}
\setcounter{biburllcpenalty}{7000}
\setcounter{biburlucpenalty}{7000}
\Urlmuskip=0mu plus 1mu

\title{Distance-based dimensionality reduction for big data literature review}
\author{Adrià Casanova Lloveras}

\begin{document}

\maketitle

\tableofcontents
\pagebreak

\section{\texttt{Rdimtools}: An R Package for Dimension Reduction
and Intrinsic Dimension Estimation \texorpdfstring{\cite{Rdimtools}}{}}
\label{sec:rdimtools}


\subsection{Abstract}

\paragraph{Original:} Discovering patterns of the complex high-dimensional data is one of the fundamental pillars of modern data science. Dimension reduction and intrinsic dimension estimation are two thematic programs that facilitate geometric characterization of the data. We present \texttt{Rdimtools}, an R package that supports 143 dimension reduction and manifold learning methods and 17 dimension estimation algorithms whose unprecedented extent makes multifaceted scrutiny of the data in one place easier. \texttt{Rdimtools} is distributed under the MIT license and is accessible from CRAN, GitHub, and its package website, all of which deliver instruction for installation, self-contained examples, and API documentation.

\paragraph{Apple Intelligence summary:} \texttt{Rdimtools}, an R package, supports 160 dimension reduction and manifold learning methods, making data analysis easier. It is available on CRAN, GitHub, and its package website.

\subsection{Key Points}

\begin{itemize}
    \item \textbf{R package}: that supports 143 dimension reduction and manifold learning methods and 17 dimension estimation algorithms.
    \item \textbf{Other libraries}: \texttt{drtoolbox} in MATLAB, \texttt{scikit-learn} in Python, a C++ template library \texttt{tapkee} with a known basis of popularity. In R, packages \texttt{dimRed}, \texttt{dyndimred}, \texttt{intrinsicDimension}.
    \item \textbf{Implementation}: mixture of R and C++ that are integrated by \texttt{Rcpp}. For numerical operations, \texttt{RcppArmadillo} is heavily used to take advantage of \texttt{Armadillo} C++ linear algebra library.
    \item  \textbf{3 function families}: \verb|do.{algorithm}|, \verb|est.{algorithm}| and \verb|aux.{algorithm}| for DR, IDE, and auxiliary functions.
    \item Downloaded 1013 times per month on average from CRAN.
    \item \textbf{Future plan}: support for out-of-memory execution in response to the increased needs for big data analysis.
\end{itemize}

\subsection{Example R Code}

\begin{minted}{r}
    # Documentation example:
    # do.idmap (Interactive Document Map)

    library(Rdimtools)

    ## load iris data
    data(iris)
    set.seed(100)
    subid = sample(1:150,50)
    X = as.matrix(iris[subid,1:4])
    lab = as.factor(iris[subid,5])
    ## let's compare with other methods
    out1 <- do.pca(X, ndim=2)
    out2 <- do.lda(X, ndim=2, label=lab)
    out3 <- do.idmap(X, ndim=2, engine="NNP")
    ## visualize
    opar <- par(no.readonly=TRUE)
    par(mfrow=c(1,3))
    plot(out1$Y, pch=19, col=lab, main="PCA")
    plot(out2$Y, pch=19, col=lab, main="LDA")
    plot(out3$Y, pch=19, col=lab, main="IDMAP")
    par(opar)
\end{minted}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Rdimtools_test.png}
    \caption{Rdimtools test output.}
    \label{fig:rdimtools_test}
\end{figure}

\pagebreak
\section{Global versus local methods in nonlinear dimensionality reduction \texorpdfstring{\cite{deSilvaTenenbaum2002}}{}}
\label{sec:deSilvaTenenbaum2002}

\subsection{Abstract}

\paragraph{Original:} Recently proposed algorithms for nonlinear dimensionality reduction fall broadly into two categories which have different advantages and disadvantages: global (Isomap), and local (Locally Linear Embedding, Laplacian Eigenmaps). We present two variants of Isomap which combine the advantages of the global approach with what have previously been exclusive advantages of local methods: computational sparsity and the ability to invert conformal maps.

\paragraph{Apple Intelligence summary:} Two new Isomap variants are presented, combining global advantages with local computational sparsity and conformal map inversion.

\subsection{Key Points}

\begin{itemize}
    \item \textbf{Introduction of LMDS} by applying it to Isomap (L-Isomap).
    \item \textbf{\textit{Landmark points}} $(n << N)$ reduce the complexity of computing:
    \begin{itemize}
        \item the distances matrix with Dijkstra's algorithm with Fibonacci heaps ($k$ = neighborhood size) from $\mathcal{O}(kN^2\log N)$ to $\mathcal{O}(knN\log N)$.
        \item MDS from $\mathcal{O}(n^3)$ to $\mathcal{O}(n^2N)$.
    \end{itemize}
    \item If $x$ is a landmark point, then the embedding given by LMDS is consistent with the original MDS embedding.
    \item If the distance matrix $D_{n,N}$ can be represented exactly by a Euclidean configuration in $\mathbb{R}^l$, and if the landmarks are chosen so that their affine span in that the configuration is $l$-dimensional (i.e. general position), then LMDS will recover the configuration exactly, up to rotation and translation.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/LMDS_3.png}
    \caption{L-Isomap is stable over a wide range of values for the sparseness parameter (the number of landmarks). Results from LLE are shown for comparision\cite{deSilvaTenenbaum2002}.}
    \label{fig:LMDS}
\end{figure}


\pagebreak
\section{\texttt{dimRed} and \texttt{coRanking} - Unifying Dimensionality Reduction in R \texorpdfstring{\cite{Kraemer2018dimRedAC}}{}}
\label{sec:Kraemer2018dimRedAC}

\subsection{Abstract}

\paragraph{Original:} “Dimensionality reduction” (DR) is a widely used approach to find low dimensional and interpretable representations of data that are natively embedded in high-dimensional spaces. DR can be realized by a plethora of methods with different properties, objectives, and, hence, (dis)advantages. The resulting low-dimensional data embeddings are often difficult to compare with objective criteria. Here, we introduce the \texttt{dimRed} and \texttt{coRanking} packages for the R language. These open source software packages enable users to easily access multiple classical and advanced DR methods using a common interface. The packages also provide quality indicators for the embeddings and easy visualization of high dimensional data. The \texttt{coRanking} package provides the functionality for assessing DR methods in the co-ranking matrix framework. In tandem, these packages allow for uncovering complex structures high dimensional data. Currently 15 DR methods are available in the package, some of which were not previously available to R users. Here, we outline the \texttt{dimRed} and \texttt{coRanking} packages and make the implemented methods understandable to the interested reader.

\paragraph{Apple Intelligence summary:} Dimensionality reduction (DR) methods create low dimensional data representations, but comparison is challenging. The \texttt{dimRed} (figure \ref{fig:dimRed_methods}) and \texttt{coRanking} R packages are introduced to address this.

\subsection{Key Points}

\begin{itemize}
    \item The difficulty in applying DR is that each DR method is designed to maintain certain aspects of the original data and therefore may be appropriate for one task and inappropriate for another. Most methods also have parameters to tune and follow different assumptions.
    \item \textbf{Software packages for other languages}:
        \begin{itemize}
            \item Python: \texttt{scikit-learn}, which contains a module for DR.
            \item Julia: \texttt{ManifoldLearning.jl} for nonlinear and \texttt{MultivariateStats.jl} for linear DR.
            \item Matlab: several toolboxes.
            \item C++: \texttt{Shogun} toolbox, which offers bindings for many high level languages (including R).
        \end{itemize}
    \item At the time (2018), no comprehensive package for R.
    \item None of the former provides means to consistently compare the quality of different methods.
    \item MDS can be seen as kPCA with kernel $x^T y$, since a distance matrix can be transformed to a matrix of inner products.
    \item \texttt{dimRed} wraps \texttt{cmdscale}.
    \item In contrast to a supervised problem, there is no natural way to directly measure the quality of any output or to compare two methods. Every
    method optimizes a different error function.
    \item \textbf{Quality criteria implemented in \texttt{coRanking}}:
        \begin{itemize}
            \item \textbf{Co-ranking matrix based measures}: the co-ranking matrix $Q$ is the 2d-histogram of the distance ranks. $q_{ij}$ is an integer which counts how many points of distance rank $j$ became rank $i$. In a perfect DR, this matrix will only have non-zero entries in the
            diagonal. In R, the co-ranking matrix can be calculated using the the \texttt{coRanking::coranking} function. The \texttt{dimRed} package contains the functions \texttt{Q\_local, Q\_global, Q\_NX, LCMC}, and \texttt{R\_NX} to calculate the above quality measures in addition to \texttt{AUC\_lnK\_R\_NX}. If $R_{NX}$ is high for low values of $K$, then local neighborhoods are maintained well; if $R_{NX}$ is high for large values of $K$, then global gradients are maintained well (see figure \ref{fig:dimRed_R_NX}).
            \item Cophenetic correlation.
            \item \textbf{Reconstruction error}: the fairest one when the method provides an inverse mapping. $\mathrm{RMSE}=\sqrt{\frac{1}{n} \sum_{i=1}^n d\left(x_i^{\prime}, x_i\right)^2}$, with $x_i' = f^{-1}(y_i) = f^{-1}(f(x_i))$.
        \end{itemize}
    \item \textbf{Test datasets}: Common ones being the 3d S-curve and the Swiss roll. Real world examples usually have more dimensions and often are much noisier and we cannot be sure if we can observe all the relevant variables. Can be retrieved with \texttt{dimRed::loadDataSet}
    \item \textbf{Main functions:} \texttt{embed, quality, plot, plot\_R\_NX}.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/dimRed_methods.png}
    \caption{DR methods implemented in \texttt{dimRed} \cite{Kraemer2018dimRedAC}.}
    \label{fig:dimRed_methods}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/dimRed_R_NX.png}
    \caption{$R_{NX}$ measures the quality of the embedding. \cite{Kraemer2018dimRedAC}.}
    \label{fig:dimRed_R_NX}
\end{figure}


\pagebreak
\section{Sparse multidimensional scaling
using landmark points \texorpdfstring{\cite{LMDS:2004}}{}}
\label{sec:LMDS:2004}

\subsection{Abstract}

\paragraph{Original:} In this paper, we discuss a computationally eﬃcient approximation to the classical multidimensional scaling (MDS) algorithm, called Landmark MDS (LMDS), for use when the number of data points is very large. The first step of the algorithm is to run classical MDS to embed a chosen subset of the data, referred to as the 'landmark points', in a low-dimensional space. Each remaining data point can be located within this space given knowledge of its distances to the landmark points. We give an elementary and explicit theoretical analysis of this procedure, and demonstrate with examples that LMDS is eﬀective in practical use.

\paragraph{Apple Intelligence summary:} Landmark MDS approximates classical multidimensional scaling for large datasets. It embeds a subset of data points, called “landmark points”, in a low-dimensional space, then locates remaining points based on their distances to these landmarks.


\subsection{Key Points}

\begin{itemize}
    \item \textbf{LMDS}: Landmark MDS.
    \item \textbf{Method}:
        \begin{enumerate}
            \item Select $n$ landmark points from $N$ data points.
            \item Apply MDS to the $n \times n$ distance matrix to obrain $L$.
            \item Embed remaining points via distance-based triangulation.
        \end{enumerate}
    \item \textbf{Complexity}: Classical MDS: $\mathcal{O}(N^2)$ storage, $\mathcal{O}(N^3)$ time. LMDS: $\mathcal{O}(nN)$ storage, lower time complexity.
    \item It has links with Isomap (L-Isomap), the Nyström method (which finds approximate solutions to a positive semi-definite symmetric eigenvalue problem using just a few of the columns of the matrix) and FastMap.
\end{itemize}


\pagebreak
\section{Comparative study for dimensionality reduction techniques for big data \texorpdfstring{\cite{SalahHenouda2020}}{}}
\label{sec:SalahHenouda2020}

\subsection{Abstract}

\paragraph{Original:} Nowadays, big data represents the solution for different type of users especially enterprises due to its huge amount of information augmented in real time. All these generated data could be described in one of big data characteristics named variety. One of the most challenging issues
for big data variety is high dimensionality because, it prevents the analysis process, demands heavy computations and adds noise to our data. The solution for this challenging issue is dimensionality reduction which minimize the dimensions and keeps only the essential information that give accurate results during analysis and decrease the computations complexity.

This paper aims to summarize and compare some of the recent methods that are used to solve the problem of high dimensionality in big data, Thereby, facilitating the process of research in this field.

\paragraph{Apple Intelligence summary:} High dimensionality in big data poses challenges, requiring dimensionality reduction to minimize dimensions and retain essential information. This paper compares recent methods for solving this problem, aiding research in the field.


\subsection{Key Points}

\begin{itemize}
    \item \textbf{Techniques Compared:}
        \begin{itemize}
            \item Classical/Modified PCA (modified to handle memory limits via row-scanning and MapReduce).
            \item Simplicial Nonnegative Matrix Tri-Factorization (SNMTF), an improved variant of NMF.
            \item Stacked Autoencoders, which yield lower reconstruction error than PCA.
            \item Linguistic Hedges Neuro-Fuzzy Classifiers with Feature Selection (LHNFCSF), a combination of neural networks and fuzzy inference systems (neuro-fuzzy) based on linguistic hedges with feature selection method.
            \item Deep Belief Networks (DBNs), which consist of multiple hidden layers where each layer is an RBM (Restricted Boltzmann Machine), which is also a class of neural networks. Each RBM is connected with two layers, a hidden layer and a visible layer, and so on.
        \end{itemize}
\end{itemize}


\pagebreak
\section{Out-of-Core Dimensionality Reduction for Large Data via Out-of-Sample Extensions \texorpdfstring{\cite{reichmann2024outofcoredimensionalityreductionlarge}}{}}
\label{sec:reichmann2024outofcoredimensionalityreductionlarge}

\subsection{Abstract}

\paragraph{Original:} Dimensionality reduction (DR) is a well-established approach for the visualization of high-dimensional data sets. While DR methods
are often applied to typical DR benchmark data sets in the literature, they might suffer from high runtime complexity and memory requirements, making them unsuitable for large data visualization especially in environments outside of high-performance computing. To perform DR on large data sets, we propose the use of out-of-sample extensions. Such extensions allow inserting new data into
existing projections, which we leverage to iteratively project data into a reference projection that consists only of a small manageable subset. This process makes it possible to perform DR out-of-core on large data, which would otherwise not be possible due to memory and runtime limitations. For metric multidimensional scaling (MDS), we contribute an implementation with out-of-sample projection capability since typical software libraries do not support it.

We provide an evaluation of the projection quality of five common DR algorithms (MDS, PCA, t-SNE, UMAP, and autoencoders) using quality metrics from the literature and analyze the trade-off between the size of the reference set and projection quality. The run-time behavior of the algorithms is also quantified with respect to reference set size, out-of-sample batch size, and dimensionality of the data sets. Furthermore, we compare the out-of-sample approach to other recently introduced DR methods, such as PaCMAP and
TriMAP, which claim to handle larger data sets than traditional approaches. To showcase the usefulness of DR on this large scale, we contribute a use case where we analyze ensembles of streamlines amounting to one billion projected instances.

\paragraph{Apple Intelligence summary:} The study compares out-of-sample DR methods with PaCMAP and TriMAP on large datasets.  A use case demonstrates the usefulness of DR on a dataset of one billion projected streamline instances.

\subsection{Key Points}

\begin{itemize}
    \item A framework for OOS extensions of DR methods similar to Interpolation MDS.
    \item Applied to PCA, MDS, t-SNE, UMAP and autoencoders.
    \item Compared to TriMap and PaCMAC, which are efficient DR methods suitable for big data.
    \item \textbf{Related work}:
    \begin{itemize}
        \item \textbf{Efficient DR algorithms}: PaCMAP (up to 4 million data points), TriMAP (up to 11 million data points), LargeVis (also millions of data points, aimed to reduce computational cost of t-SNE). Note that t-SNE and UMAP fail at 1 million data points.
        \item \textbf{Parametric DR methods}: learn explicit maps between the high- and low-dimensional space.  Hinterreiter et al. \cite{ParaDime} introduced a framework making parametric extensions generally available for DR techniques.
        \item \textbf{GPU porting}: of t-SNE, UMAP and metrics computations such as trustworthiness. Reichmann et al. also parallelize metric MDS.
        \item \textbf{OOS extensions}: Bengio et. al. \cite{Bengio2003OOS} introduced an OOS framework by learning the corresponding eigenfunctions. Gisbrecht et. al. \cite{Gisbrecht2012OutofsampleKE} proposed three kernel-based methods and \cite{Zhang2021OOS} improved them.
    \end{itemize}
    \item \textbf{Computational framework}: see figure \ref{fig:OOS_algorithm}. Details and sematics of the $\beta$ parameters depends on the DR method. As an example, see figure \ref{fig:OOS_methods}.
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/OOS_algorithm.png}
        \caption{OOS projection framework proposed in \cite{reichmann2024outofcoredimensionalityreductionlarge}.}
        \label{fig:OOS_algorithm}
    \end{figure}

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/OOS_methods.png}
        \caption{DR methods used in the evaluation of the OOS framework proposed in \cite{reichmann2024outofcoredimensionalityreductionlarge}.}
        \label{fig:OOS_methods}
    \end{figure}

    \item \textbf{OOS extensions} used:
    \begin{itemize}
        \item \textbf{PCA} and the \textbf{autoencoder} learn a parametric mapping and give an explicit function to map the data.
        \item For \textbf{metric MDS}, they minimize stress for a single point while keeping all others fixed (\textit{single scaling}) to perform the OOS projection of a single point. The gradient for an OOS point $x^{\prime}$ with respect to the points $x_i$ of the reference set $X_a$ and corresponding low-dimensional points $y_i$ is given as $\delta=\sum_i\left(1-d\left(x^{\prime}, x_i\right) /\left\|y^{\prime}-y_i\right\|\right) \cdot\left(y^{\prime}-y_i\right)$, with dissimilarity function $d(\cdot, \cdot)$.
        \item A similar mechanism can be used in \textbf{t-SNE} and \textbf{UMAP}.
    \end{itemize}
    \item \textbf{Datasets} of up to 50,000,000 data points were used to evaluate the OOS framework, as can be seen in figure \ref{fig:OOS_datasets}. That being said, a non-consumer very powerful system was used; it consisted of an AMD Ryzen Threadripper PRO 3995WX with 64 CPU cores, an NVIDIA RTX A6000 GPU with 48 GiB of VRAM, and 251 GiB of RAM.
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.8\textwidth]{figures/OOS_datasets.png}
        \caption{Datasets used in the evaluation of the OOS framework proposed in \cite{reichmann2024outofcoredimensionalityreductionlarge}.}
        \label{fig:OOS_datasets}
    \end{figure}
    
    \item \textbf{Metrics} used:
    \begin{itemize}
        \item \textbf{Global}: \textit{stress} (calculation in batches in the GPU), \textit{Pearson correlation coefficient} between the flattened low- and high-dimensional distance vectors.
        \item \textbf{Local}: \textit{KNN precision}, \textit{trustworthiness} (calculation in batches in the GPU).
    \end{itemize}
    
    \item \textbf{Computational complexities}:
    \begin{itemize}
        \item Runtime (RT) complexity:
        $\mathcal{O}\Bigl(\Phi^{\mathrm{RT}}\bigl(n_{\mathrm{ref}}\bigr)\Bigr)
        +\mathcal{O}\Bigl(\Phi_\beta^{\mathrm{RT}}\bigl(n_{\mathrm{batch}}\bigr)
        \cdot \text{batch\_count}\Bigr)$
        \item Memory (M) complexity of a batch projection:
        $\mathcal{O}\Bigl(\Phi^{\mathrm{M}}\bigl(n_{\mathrm{ref}}\bigr)\Bigr)
        +\mathcal{O}\Bigl(\Phi_\beta^{\mathrm{M}}\bigl(n_{\mathrm{batch}}\bigr)\Bigr)$
        \item Metric MDS: $\Phi\bigl(n_{\mathrm{ref}}\bigr)$ is $\mathcal{O}(n_{\mathrm{ref}}^2)$ in RT and M. $\Phi_{\beta}\bigl(n_{\mathrm{batch}}\bigr)$ has $\mathcal{O}(n_{\mathrm{ref}} \cdot n_{\mathrm{batch}})$ time complexity (and I'd argue it is $\mathcal{O}(n_{\mathrm{batch}}^2)$ in memory).
        \item t-SNE: same as metric MDS, except for M of $\Phi_{\beta}$.
        \item UMAP: $\Phi\bigl(n_{\mathrm{ref}}\bigr)$ is $\mathcal{O}(n_{\mathrm{ref}}^{1.14})$ in T and $\Phi_{\beta}\bigl(n_{\mathrm{batch}}\bigr)$ is like in MDS.
        \item Autoencoders: $\Phi$ has linear TR and its M depends on the architecture. $\Phi_{\beta}$ is approximately linear.
        \item PCA: $\Phi$ is linear with respect to $n_{\mathrm{ref}}$ and dimensionality. $\Phi_{\beta}$ is approximately linear.
    \end{itemize}

    \item \textbf{Implementation}: in Python with \texttt{Numba}(see section \ref{sec:Numba}) compiled functions for performance-critical sections. They used the UMAP implementation of the umap-learn library, the openTSNE library for t-SNE, scikit-learn for PCA, and keras for the autoencoder. For MDS, they provided their own implementation (see \verb$code/ReichmanHagele2024_MDS/mds_demo.py$).
    \item \textbf{Experiments results}:
    \begin{itemize}
        \item \textbf{Quality}: PCA and MDS are consistent very early on (starting with a ref. set size of 128). t-SNE and UMAP overfit and generate clusters on small ref. sets, although they perform well on local metrics. Autoencoders is the most inconsistent method, but in general when the ref. set size increases, global metrics worsen and local ones improve.
        \item \textbf{Runtime}: PCA and MDS are consistent with theory. t-SNE shows a below quadratic time complexity of $\Phi$, why? Moreover, time complexity of $\Phi_{\beta}$ seems constant with respect to $n_{\mathrm{ref}}$, why? UMAP is significantly slower in high-dimensional datasets. The dependance on batch size is inconsistent in most methods, even though the general trend is to be sub-linear.
    \end{itemize}
    \item \textbf{Comparison to large-scale DR methods}: when comparing UMAP with the proposed OOS framework against classical UMAP, PaCMAP and TriMap, PaCMAP is consistently better in terms of quality metrics, although the OOS framework is capable of reducing runtime significantly at the expense of quality. In previous work, UMAP failed to transform datasets of 1 million rows; how did the authors manage to run it on datasets of 2, 5 and 11 million data points (Tornado, KDD Cup '99 and Higgs, respectively)?

    \item They finally show a use case with 1 billion streamlines generated on the \textit{fluid simulation ensemble for machine learning}.
    
    \item \textbf{Final discussion}:
    \begin{itemize}
        \item Even though results depend on the DR technique, runtime can always be substantially reduced by using smaller reference set sizes.
        \item The authors recommend testing the reference projection before applying the OOS projection to the whole data set.
        \item Tuning the hyperparameters of the DR methods should improve results.
        \item An informed selection of the reference set instead of a random one could improve results as well.
        \item Another limitation of the proposed framework is that relationships between OOS points are ignored and that $\Phi_{\beta}$ is not updated with OOS points.
    \end{itemize}
\end{itemize}


\pagebreak
\section{Classical MDS benchmarked in R}

In this section we aim to test the limits of \texttt{Rdimtools::do.mds}. Specifically, we will run Rdimtools's implementation of Classical MDS on datasets of increasing number of observations and dimensionality in order to test its limitations as well as its complexity. We will also compare it with \texttt{dimRed::embed(.method="MDS", ...)}.

Code, data and figures of the experiment can be found in \verb$code/MDS_benchmark_R_lib$. As for the system used, tests were performed on a Macbook Pro (14-inc, Nov 2023) with 16 GB of RAM and the Apple M3 chip.

In our first experiment (see figure \ref{fig:MDS_Rdimtools}), we run \texttt{Rdimtools::do.mds} on datasets consisiting of four isotropic gaussian blobs with random covariance matrices randomly centered in a $d$-dimensional space with sizes ranging from 100 to 100000. We let $d$ be 8, 32 and 64. However, for all dimensionalities tested, R crashed when the dataset had at least 56234 rows. This result shows the memory limitation of classical MDS, which is reached in datasets with sizes of the order of $10^5$ for modern consumer-grade systems.

Regarding time complexity, classical MDS shows quadratic growth with respect to number of individuals and linear growth in dimensionality, although we cannot be absolutely sure of the latter because only three dimensionalities were tested.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/MDS_Rdimtools.pdf}
    \caption{Runtime (s) of \texttt{Rdimtools::do.mds} averaged over 20 experiments. The tests were performed on datasets consisiting of four isotropic gaussian blobs with random covariance matrix and randomly centered in a $d$-dimensional space with sizes ranging from 100 to 31623. We let $d$ be 8, 32 and 64.}
    \label{fig:MDS_Rdimtools}
\end{figure}

Next, we compared the implementations of classical MDS in libraries \texttt{Rdimtools} and \texttt{dimRed}, since \texttt{Rdimtools} is said to be very efficient while \texttt{dimRed} just wraps the \texttt{cmdscale} method \cite{Kraemer2018dimRedAC}. As can be seen in figure \ref{fig:MDS_dimRed_vs_Rdimtools}, our tests corroborate the results shown in \cite{Rdimtools}, since \texttt{Rdimtools::do.mds} has a lower time complexity than \texttt{dimRed::embed(.method="MDS", ...)}, resulting in runtime improvements of 10x for datasets with 100 rows and up to 1000x for samples with 10000 observations. Indeed, we did not test \texttt{dimRed::embed(.method="MDS", ...)} on larger datasets because, when size equaled 1000 rows, each experiment took around 15 minutes to run on our system.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/MDS_dimRed_vs_Rdimtools.pdf}
    \caption{Runtime (s) of \texttt{Rdimtools::do.mds} and \texttt{dimRed::embed(.method="MDS", ...)} averaged over 20 experiments. Both methods were tested on the same datasets, consisiting of four isotropic gaussian blobs with random covariance matrix and randomly centered in an 8-dimensional space with sizes ranging from 100 to 10000 for \texttt{dimRed::embed(.method="MDS", ...)} and from 100 to 31623 for \texttt{Rdimtools::do.mds}.}
    \label{fig:MDS_dimRed_vs_Rdimtools}
\end{figure}

\pagebreak
\section{DR Python packages}

\subsection{Key Points}

\begin{itemize}
    \item To our knowledge, there is no comprehensive Python package that implements many DR techniques akin the R library \texttt{Rdimtools} (see section \ref{sec:rdimtools}).
    \item To show the consequences of this in an example, classical MDS is only implemented in \texttt{pyseer} (through \texttt{pyseer.cmdscale}), a package for microbial pangenome-wide association studies \cite{pyseer}.
    \item The package that contains the most DR methods is \texttt{scikit-learn}, specifically in its \texttt{manifold} module \cite{pedregosa2011scikit}. It implements: Isomap, LLE, Laplacian Eigenmaps, t-SNE and non-classical MDS (both \texttt{sklearn.manifold.MDS} and \texttt{sklearn.manifold.smacof} can perform metric and non-metric MDS). Moreover, \texttt{sklearn.manifold.trustworthiness} measures to what extent the local structure is retained when dimensionality is reduced. Hence, the total amount of methods present in \texttt{sklearn.manifold} cannot be compared to the 143 of \texttt{Rdimtools}.
    \item Another remarkable, although differently oriented, library is \texttt{direpack}, which implements a set of modern statistical dimension reduction techniques including projection pursuit, sufficient dimension reduction, and robust M estimators. It also includes regularized regression estimators, pre-processing utilities, plotting functionality, and cross-validation utilities, all consistent with the scikit-learn API \cite{direpack}. Nonetheless, these methods are outside the scope of our thesis.
    \item Other, more specific, libraries that implement DR methods in Python are:
    \begin{itemize}
        \item \texttt{umap-learn}: the library that introduced UMAP \cite{2018arXivUMAP}.
        \item \texttt{MulticoreTSNE}: a multicore modification of Barnes-Hut t-SNE with Python CFFI-based wrappers \cite{Ulyanov2016}. Barnes-Hut is a tree-based algorithm that can be used to accelerate t-SNE up to $\mathcal{O}(N\log N)$ \cite{JMLR:v15:vandermaaten14a}.
        \item \texttt{fitsne}: Fast Fourier Transform-accelerated Interpolation-based t-SNE \cite{fitsne}.
        \item \texttt{OpenTSNE}: incorporates the latest improvements to the t-SNE algorithm, including the ability to add new data points to existing embeddings, massive speed improvements, enabling t-SNE to scale to millions of data points and various tricks to improve global alignment of the resulting visualizations \cite{Policar2024}. The team behind \texttt{OpenTSNE} benchmarked the here described t-SNE Python implementations and showed that the fastest one in general is \texttt{fitsne}, although \texttt{OpenTSNE} is equally efficient on multicore systems (see figure \ref{fig:python_tsne_benchmarks}) \cite{Policar2024}.
    \end{itemize}
    \item Moreover, other libraries focused on certain fields like genomics (\texttt{phate}) or NLP (\texttt{gensim}) implement DR methods for specific tasks and scenarios in their topics \cite{phate, rehurek_lrec}.
\end{itemize}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/python_tsne_benchmarks.png}
    \caption{Benchmark of t-SNE Python implementations \cite{Policar2024}.}
    \label{fig:python_tsne_benchmarks}
\end{figure}

\pagebreak
\section{The \texttt{Numba} Python package}
\label{sec:Numba}

\texttt{Numba} \cite{lam2015numba} is an open‐source just-in-time (JIT) compiler that translates a subset of Python and \texttt{Numpy} code into optimized machine code using LLVM.

It accelerates numerical computations on CPUs and GPUs by enabling parallelization and vectorization with minimal code modifications. To gain the most out of it, computations should use loops, \texttt{Numpy} functions and \texttt{Numpy} broadcasting.

\subsection{How does \texttt{Numba} work?}

As described in its documentation \cite{lam2015numba}, \texttt{Numba} reads the Python bytecode for a decorated function and combines this with information about the types of the input arguments to the function. It analyzes and optimizes your code, and finally uses the LLVM compiler library to generate a machine code version of your function, tailored to your CPU capabilities. This compiled version is then used every time your function is called.

\pagebreak
\section{Title \texorpdfstring{TEXstring}{}}
\label{sec:}

\subsection{Abstract}

\paragraph{Original:}

\paragraph{Apple Intelligence summary:}


\subsection{Key Points}

\begin{itemize}
    \item 
\end{itemize}


\pagebreak
\printbibliography

\end{document}