\section{Experimentation and evaluation of the proposal}
\label{sec:experimentation-and-evaluation}

The goal of this Section is to present the experiments we have conducted on the divide-and-conquer DR algorithm to assess its quality and performance. Following the methodology described in Section \ref{sec:experiment-methodology-parameter-tuning}, we were able to corroborate the space and time complexity of the procedure and understand better its embedding mechanism. We chose the Swiss roll and MNIST datasets for the tests because of their popularity, which makes it easier to compare our method with others in the literature, and because of how well Isomap and t-SNE embedded them. That allowed us to go even further and successfully unfold a $10^8$ points Swiss roll with divide-and-conquer Isomap. On the other hand, the classification task in the MNIST dataset proved more complicated to our algorithm, which was slower and separated digits worse than \verb|openTSNE|'s implementation of t-SNE.

The experimentation and evaluation process also provided us some insights on the bare SMACOF, LMDS, Isomap and t-SNE techniques. For example, we realized that LMDS, a nonlinear method intended to overcome the limitations of the SMACOF algorithm, was not able to unfold the Swiss roll dataset. Even after having tuned the $k$ and $\tau$ parameters (see Algorithm \ref{alg:LMDS}), its embedding was porous and irregular instead of uniform and rectangular (see Figure \ref{fig:LMDS-swiss-roll}). Further research on this problem could lead to the development of a variation of LMDS that reduces the dimensionality of the Swiss roll better.

Additionally, we will describe the computer system we performed the tests on. Even though our main development computer was a  Macbook Pro (14-inc, Nov 2023) with 16 GB of RAM and the Apple M3 chip, we noticed that the \verb|concurrent.futures| module, which parallelized the execution of Algorithm \ref{alg:DivideConquer}, did not work in Mac computers with ARM chipsets. Therefore, we ended up using a Windows system. Specifically, our PC was an Asus ROG G513QM-HF026 laptop with the AMD Ryzen 7 5800H CPU, 16 GB of DDR4-3200MHz RAM, an SSD M.2 NVMe PCIe 3.0 and an NVIDIA RTX 3060 GPU.

Concluding, in Section \ref{sec:initial-runtime-benchmarks}, we will present the benchmarks performed on Isomap and t-SNE. In Section \ref{sec:overheads-DR}, we measure the dividing, embedding and merging steps of divide-and-conquer DR separately to detect possible computation overheads. Finally, in Sections \ref{sec:dc-SMACOF}, \ref{sec:dc-LMDS}, \ref{sec:dc-Isomap} and \ref{sec:dc-t-SNE}, we go through a qualitative analysis of the divide-and-conquer DR algorithm on the previously discussed datasets and dimensionality-reduction methods.


\subsection{Runtime benchmarks of divide-and-conquer Isomap and divide-and-conquer t-SNE}
\label{sec:initial-runtime-benchmarks}

Isomap was the first DR method we implemented into our divide-and-conquer framework, so it also was the first method we benchmarked. Later, we measured the runtime of divide-and-conquer t-SNE as well, since t-SNE is the most popular DR method nowadays. We tested divide-and-conquer Isomap on the Swiss roll dataset with different parameter combinations and with parallel and serial computation. However, in all tests, the number of connecting points for the Procrustes transformation was 100. We chose this number because, based on the thesis directors' experience on big data MDS \citep{Delicado2024}, it guaranteed good links between partitions' embeddings and efficient computations when $1,000 \leq l \leq 10,000$. Figure \ref{fig:Isomap-benchmark} shows the average runtimes of 20 experiments with different sets of parameters and dataset sizes. Parameters were previously tuned to ensure embeddings would preserve the structure of the data. As described in Section \ref{sec:experiment-methodology-parameter-tuning}, we applied bare Isomap to Swiss rolls of 1,000, 3,162 and 10,000 points, following a logarithmic sequence. In fact, the leftest point in every graph corresponds to tests where $l=n$, so only one partition was identified and data was not divided nor merged. This explains the shorter runtimes in these tests. Notice that we increased the number of neighbors $k$ for larger values of $l$ because partitions were denser.

After some experimentation, we realized that if $l=3162$ and $k=10$, the embedding was nearly perfect no matter the amount of individuals (see Figure \ref{fig:Isomap-huge}). Meanwhile, when $l=10,000$ and $k=15$, quality was similar and time was significantly larger, so we used $l=3162$ and $k=10$ for the largest datasets. This way, we managed to embed $10^8$ three-dimensional points into the Euclidean plane in about 3 hours. Hence, we showed that divide-and-conquer Isomap is capable of handling arbitrarily large datasets on a standard computer while maintaining the quality of the embedding.

Regarding parallelization, we can observe in Figure \ref{fig:Isomap-benchmark} that it effectively reduces the time complexity of divide-and-conquer DR, although its overhead slows down the algorithm when $n \leq 10^4$. Overall, results show that divide-and-conquer Isomap is linear in time with respect to $n$.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Isomap-benchmark.png}
    \caption{Runtime (s) of divide-and-conquer Isomap averaged over 20 experiments. Tests were performed on datasets generated on the Swiss roll manifold with sizes ranging from $10^3$ to $10^8$. Data was embedded into $\mathbb{R}^2$ with different parameter combinations and $c=100$. Runtime in parallel and serial execution is also compared.}
    \label{fig:Isomap-benchmark}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Isomap-huge.png}
    \caption{Bidimensional embedding of a $10^8$ points Swiss roll dataset computed by divide-and-conquer Isomap with $k=10, \, l=3,162$ and $c=100$. Color represents the angle of rotation along the Swiss roll spiral.}
    \label{fig:Isomap-huge}
\end{figure}

Afterward, we tested divide-and-conquer t-SNE on the same datasets (see Figure \ref{fig:t-SNE-benchmark}). However, t-SNE performed notably slower than Isomap on the Swiss roll, so we only run its divide-and-conquer variation with one parameter combination, $c = 100, \, l=1,000, \, Perp=30$ and $n\_iter=250$, where $n\_iter$ is the number of iterations carried out to minimize the Kullback-Leibler divergence \citep{Kullback1951}.

Even though divide-and-conquer t-SNE is about two orders of magnitude slower than divide-and-conquer Isomap, time complexity is linear as well, proving the expected results. The quality of the embedding, on the other hand, is very low. See Figure \ref{fig:t-SNE-huge} to observe that the structure of the data is broken into separate parts and the spiral shape is not unfolded. The curved shapes in the embedding show that this problem is caused by t-SNE itself, which does not comprehend the intrinsic dimensionality of the data.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tSNE-benchmark.png}
    \caption{Runtime (s) of divide-and-conquer Isomap and divide-and-conquer t-SNE averaged over 20 experiments. Tests were performed on datasets generated on the Swiss roll manifold with sizes ranging from $10^3$ to $10^8$. Data was embedded into $\mathbb{R}^2$ with different parameter combinations and $c=100$.}
    \label{fig:t-SNE-benchmark}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/t-SNE-swiss-roll-huge.png}
    \caption{Bidimensional embedding of a $10^6$ points Swiss roll dataset computed by divide-and-conquer t-SNE with $l=1,000, \, c=100, \, Perp=30$ and $n\_iter=250$. Color represents the angle of rotation along the Swiss roll spiral.}
    \label{fig:t-SNE-huge}
\end{figure}

\subsection{Analysis of possible overheads in divide-and-conquer DR}
\label{sec:overheads-DR}

One reasonable question to formulate about the divide-and-conquer approach is whether splitting the data and merging the resulting embeddings constitute a significant computational cost in comparison to reducing each partition's dimensionality. Theoretically, fractionating the data should be rapid. As for the merging of partitions' embeddings, in Section \ref{sec:specification-and-design-of-the-solution} we depicted how we align and overlap them with Procrustes transformations between each embedding and one in specific (the first one). We intentionally find a rigid transformation with a random subset of only $c < l$ points to accelerate its computation, and then multiply the full partition's embedding with the $q\times q$ matrix of the transformation. Therefore, as discussed in Section \ref{sec:Python-implementation-of-divide-and-conquer-DR}, the overhead of merging each embedding should be $\mathcal{O}(q^2(l+q))$.

Table \ref{tab:dc-overhead} shows the results of an experiment where each part of the divide-and-conquer DR algorithm was independently timed. We uniformly sampled 5,000 points from the MNIST dataset and embedded them into the Euclidean plane with divide-and-conquer SMACOF. The arguments used were $l=1000,\, c=100,\, n\_iter = 300,\, \varepsilon = 0.001$. Even though it being a single experiment, we selected commonly used data and results were very clear.  Hence, the following conclusions can be extrapolated to other settings. As it was expected, neither partitioning the dataset nor aligning partial embeddings entail a noteworthy overhead in divide-and-conquer DR. Indeed, the prior and the latter were about 5,506 times and 50,710 times swifter than embedding all partitions with SMACOF, respectively.

\begin{table}
    \centering
    \caption{Runtime (s) of each step of divide-and-conquer SMACOF on a 5000-point random subset of MNIST. The arguments used were $l=1000,\, c=100,\, n\_iter = 300,\, \varepsilon = 0.001$.}
    \begin{tabular}{lccc}
        \toprule
        Operation    & Divide & Embed & Merge \\
        \midrule
        Duration (s) & $6.88 \times 10^{-3}$ & 37.89 & $7.47 \times 10^{-4}$ \\
        \bottomrule
    \end{tabular}
    \label{tab:dc-overhead}
\end{table}

\subsection{Divide-and-conquer SMACOF}
\label{sec:dc-SMACOF}

\subsubsection{Swiss roll}
\label{sec:dc-SMACOF-swiss-roll}

In this experiment, we applied bare and divide-and-conquer SMACOF to Swiss rolls of different sizes. The largest dataset the bare method could handle ended up having 7,500 individuals and taking about 14 minutes to calculate. We show in Figure \ref{fig:SMACOF-swiss-roll-7500} the results of the test.

The spiraled shape of both embeddings confirm that SMACOF cannot identify the intrinsic bidimensionality of the Swiss roll. Moreover, SMACOF condenses more the outer part of the manifold than the inner one and generates rugged edges. Regarding the qualitative differences between bare and divide-and-conquer versions, there is not much to be said. Divide-and-conquer SMACOF properly aligns the partial configurations and obtains a similar, more rugged, embedding than bare SMACOF.

To conclude, even though the SMACOF algorithm is not capable of properly embedding the Swiss roll, we can see a clear advantage in using it with the divide-and-conquer framework on other datasets. Indeed, divide-and-conquer SMACOF is about 19 times faster than the bare method while returning a similar low-dimensional configuration.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/SMACOF-swiss-roll-7500.png}
    \caption{Comparison of the bidimensional embeddings of a 7,500 points Swiss roll dataset by bare (left) and divide-and-conquer (right) SMACOF. The arguments used were $n\_iter = 300,\, \varepsilon = 0.001$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Color represents the angle of rotation along the Swiss roll spiral.}
    \label{fig:SMACOF-swiss-roll-7500}
\end{figure}


\subsubsection{MNIST}
\label{sec:dc-SMACOF-MNIST}

The goal of embedding the MNIST dataset into an Euclidean plane usually is to classify and identify all digits represented in the high-dimensional data. In that sense, this is a common use case of dimensionality reduction: extracting the underlying structure of the data. With the SMACOF algorithm, we sampled 5,000 images from MNIST and tuned the $n\_iter$ and $\varepsilon$ parameters as well as $l$ and $c$ to separate as well as possible the low-dimensional configurations of all digits. In other words, we calibrated all parameters to achieve the best possible visual classification of numbers.

Figure \ref{fig:SMACOF-MNIST-kde} shows the result of bare and divide-and-conquer SMACOF. Except for a small cluster of eights, they are very similar and both commit the same problems and achievements. Most digits are clearly separated, except for 4, 7 and 9, which have similar shapes and therefore this behavior could be expected. Additionally, 5 falls between 2 and 8, since some hand-drawn fives resemble upside-down twos and others might look like eights. The former misclassification might denote a difficulty in comprehending the vertical orientation of pictures in SMACOF.

In order to better understand the differences between embeddings, we compared in Figure \ref{fig:SMACOF-MNIST-coords} each coordinate of both configurations against each other. The plots resemble a straight line with $x=y$ equation. There is a cloud of points around the plot and the line of the vertical coordinate is thicker than that of the horizontal one, but overall, and coinciding with Figure \ref{fig:SMACOF-MNIST-kde}, the embeddings of bare and divide-and-conquer SMACOF are very similar.

Finally, we computed the Pearson correlation between bare and divide-and-conquer SMACOF in the projected dimensions. In the first dimension it was 0.900 and in the second one it was 0.879. Values close to 1 corroborate a significant resemblance between both embeddings. This evidence indicates that the fastest method should be applied in similar datasets. Meanwhile, our measurements conclude that the divide-and-conquer variation performed about 6 times faster (see Figure \ref{fig:SMACOF-MNIST-kde}) than the traditional one, so it would be more preferable in related applications.

The main goal of divide-and-conquer DR is to reduce the dimensionality of big datasets without keeping their full distance matrices in main memory. In order to assess this feature, we performed one more experiment on divide-and-conquer SMACOF in the MNIST dataset. Specifically, we embedded it all with the parameter values tuned in the previous experiment and obtained the bidimensional configuration depicted in Figure \ref{fig:SMACOF-MNIST-huge}. On the other hand, the execution of bare SMACOF crashed due to an absence of enough main memory. We can observe in Figure \ref{fig:SMACOF-MNIST-huge} that the embedding of the whole dataset took 49 minutes to compute and is alike to that of the 5,000 points sampled subset. Even though the clusters of seven's and nine's images are a bit distinct, which could be due to a misrepresentation in the sampled data, we may notice that divide-and-conquer SMACOF is consistent among related high-dimensional data configurations.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/SMACOF-MNIST-kde.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST by bare (left) and divide-and-conquer (right) SMACOF. The arguments we used were $n\_iter = 300,\, \varepsilon = 0.001$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:SMACOF-MNIST-kde}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/SMACOF-MNIST-coords.png}
    \caption{Scatter plots of first (left) and second (right) coordinates in the embeddings represented in Figure \ref{fig:SMACOF-MNIST-kde}. SMACOF is compared against divide-and-conquer SMACOF.}
    \label{fig:SMACOF-MNIST-coords}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/SMACOF-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the whole MNIST dataset by divide-and-conquer SMACOF. The arguments used were $n\_iter = 300,\, \varepsilon = 0.001, \, l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:SMACOF-MNIST-huge}
\end{figure}

\subsection{Divide-and-conquer LMDS}
\label{sec:dc-LMDS}

\subsubsection{Swiss roll}

When we applied bare LMDS to the Swiss roll dataset, we did not expect the embedding we obtained. Figure \ref{fig:LMDS-swiss-roll} shows the best configuration in $\mathbb{R}^2$ in the tuning process we run. As it can be seen, points tend to cluster in filaments and leave gaps between them. Moreover, the top and bottom sides of the configuration are bent inwards, effectively bringing together points with the same angle of rotation in the manifold's spiral, something that does not happen at the left or right edges. Given that LMDS is a modification of the SMACOF algorithm that address nonlinearities in the data, we expected it would properly unfold the spiral of the Swiss roll. However, while it does understand the intrinsic dimensionality of the manifold better than SMACOF (see Section \ref{sec:dc-SMACOF-swiss-roll}), it does not preserve the uniformity of the data nor its global shape.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/LMDS-swiss-roll.png}
    \caption{Bidimensional embedding of a 1,000 points Swiss roll dataset computed by LMDS with $k=10$ and $\tau = 0.1$. Color represents the angle of rotation along the Swiss roll spiral.}
    \label{fig:LMDS-swiss-roll}
\end{figure}

\subsubsection{MNIST}

In order to experiment with LMDS and the MNIST dataset, we considered the same 5,000 points subset of the data used in Section \ref{sec:dc-SMACOF-MNIST} and tuned the parameters of the bare and divide-and-conquer techniques. In Figure \ref{fig:LMDS-MNIST}, both planar optimal configurations are represented. Bare LMDS widely separates most digits, except for the commonly confused 4, 7 and 9. On the other hand, divide-and-conquer LMDS condenses the data into a narrower region of the plane, so the clusters of digits inevitably overlap. Apart from this problematic behavior, relative positions of digits' clusters are similar in both embeddings.

When we observe the dimensional correlation between bare's and divide-and-conquer's outcomes, we may notice they are very high: 0.949 in the first dimension and 0.821 in the second one. These values could be explained by the fact that the digits' clusters in both embeddings have similar relative positions.

Given the poorer embedding quality of divide-and-conquer LMDS, it might not be worth applying it to small datasets for classification. That being said, our framework achieves a non-contemptible 1,536\% acceleration to the computing time of LMDS. Furthermore, Figure \ref{fig:LMDS-MNIST-huge} shows that, contrary to bare LMDS, it can also tackle the whole MNIST dataset in a conventional computer and a reasonable time (about 35 minutes).

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/LMDS-MNIST.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST by bare (left) and divide-and-conquer (right) LMDS. The arguments used were $k=10,\, \tau = 1$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:LMDS-MNIST}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/LMDS-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the whole MNIST dataset by divide-and-conquer LMDS. The arguments used were $k=10,\, \tau = 1, \, l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:LMDS-MNIST-huge}
\end{figure}

\subsection{Divide-and-conquer Isomap}
\label{sec:dc-Isomap}

We discussed the embedding of divide-and-conquer Isomap for Swiss rolls of different sizes in Section \ref{sec:initial-runtime-benchmarks}. As \citet{Spiwokv2007} first discovered, Isomap flawlessly reduces the dimensionality of the Swiss roll manifold. Moreover, divide-and-conquer Isomap achieves the same embedding quality in big datasets when $k=10,\, l=3,162$ and $c=100$.

\subsubsection{MNIST}

Figure \ref{fig:Isomap-MNIST} shows that the big success of Isomap on the Swiss roll is not repeated on the MNIST dataset. When it comes to classification, bare and divide-and-conquer Isomap, overlap many clusters, thus showing a poor understanding of the intrinsic structure of the data. As usual, digits 4, 7 and 9 are very often confused. On bare Isomap, we also observe intersections between clusters of: zeroes and sixes; ones and twos; fives and sixes; and twos and eights. Divide-and-conquer Isomap enlarges most intersections and also confuses the clusters of twos and sevens, although it separates zeroes and sixes.

When we look at the Pearson correlation between each method across all dimensions, we observe they do not convey the previously observed qualitative differences. In particular, the correlation between the first dimension is 0.920 and between the second one is 0.862. Finally, note that, even though we are using a very fast implementation of Isomap that surpasses SMACOF and LMDS in performance, the divide-and-conquer approach makes it 4 times faster.

As we did with SMACOF and LMDS, we considered the previously tuned parameters of divide-and-conquer Isomap and embedded the whole MNIST dataset. Figure \ref{fig:Isomap-MNIST-huge} plots the kernel density estimation of the outcome. Overall, even if clusters are shaped differently than in the 5,000 points configuration (see Figure \ref{fig:Isomap-MNIST}), it essentially confuses  digits the same way. Runtime is remarkably fast, anyway, since divide-and-conquer Isomap embedded 345,035 pictures into $\mathbb{R}^2$ in only two minutes.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/Isomap-MNIST.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST by bare (left) and divide-and-conquer (right) Isomap. The arguments used were $k=5$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:Isomap-MNIST}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Isomap-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the whole MNIST dataset by divide-and-conquer Isomap. The arguments used were $k=5, \, l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:Isomap-MNIST-huge}
\end{figure}

\subsection{Divide-and-conquer t-SNE}
\label{sec:dc-t-SNE}

Since t-SNE is especially well suited for visualization tasks, this method is known to greatly group digits in MNIST \citep{Vandermaaten2008}. On the other hand, in Section \ref{sec:initial-runtime-benchmarks} we presented its embedding of the Swiss roll dataset. From Figure \ref{fig:t-SNE-huge}, we concluded that partial configurations were curved and inconsistent, so Procrustes transformations were not able to merge then into a correct nor regular global strucutre.

\subsubsection{MNIST}

Even though t-SNE is very used nowadays \citep{Wattenberg2016}, we have detected inconsistencies when embedding similar datasets. This could be fatal for divide-and-conquer DR because partial embeddings are only aligned with rigid motions, so geometric differences could not be vanished in the merging step. Hence, before comparing bare and divide-and-conquer t-SNE, we considered two non-intersecting partitions of the MNIST dataset with 172,517 points each and embedded them into the Euclidean plane. Figure \ref{fig:t-SNE-MNIST-partitions} shows the kernel density estimation of every digit's pictures in both partitions. Even though most clusters are similar between partitions, twos and fours are swapped in partition 2. Therefore, these digits perfectly classified by bare t-SNE, would be confused by the divide-and-conquer variation were these partitions taken. What this means is that, as seen with Isomap (see Figure \ref{fig:t-SNE-huge}), divide-and-conquer t-SNE will probably diminish the local and global quality of MNIST's embedding.

Figure \ref{fig:t-SNE-MNIST} and Figure \ref{fig:t-SNE-MNIST-huge} show the embeddings of bare and divide-and-conquer t-SNE on the previously sampled 5,000 points of MNIST and the whole dataset, respectively. As expected, bare t-SNE completely separates different digits, while divide-and-conquer t-SNE overlaps many of their clusters, especially in the smaller dataset. The dimension correlation between both methods on the 5,000 points dataset measures this disagreement with a 0.328 correlation in the second dimension, although the first dimensions have a correlation of 0.817.

Moreover, as explained in Section \ref{sec:openTSNE}, we are using a very performant implementation of t-SNE provided by \verb|openTSNE|, which can tackle big datasets on its own. This results in the bare method being faster than our divide-and-conquer approach, which had not happened with SMACOF, LMDS nor Isomap. Even though in the 5,000 points subset divide-and-conquer t-SNE was 1.3 times faster than t-SNE, in the full dataset, \verb|openTSNE|'s implementation was 144 times faster than ours. Concluding, t-SNE has received many specific optimizations that make it very hard to improve.

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/t-SNE-MNIST-partitions.png}
    \caption{Kernel density estimation of the bidimensional embeddings of two halves of the MNIST dataset. Data was randomly ordered before being splitted. The DR method used was divide-and-conquer t-SNE with $Perp=30$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:t-SNE-MNIST-partitions}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/t-SNE-MNIST-5000.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST by bare (left) and divide-and-conquer (right) t-SNE. The arguments used were $Perp=30$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:t-SNE-MNIST}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.95\textwidth]{figures/t-SNE-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the whole MNIST dataset by bare (left) and divide-and-conquer (right) t-SNE. The arguments used were $Perp=20, \, n\_iter=100$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Contour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:t-SNE-MNIST-huge}
\end{figure}
