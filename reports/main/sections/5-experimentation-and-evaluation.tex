\section{Experimentation and evaluation of the proposal}
\label{sec:experimentation-and-evaluation}

The goal of this section is to present the experiments we have conducted on the divide-and-conquer DR algorithm to assess its quality and performance. Following the methodology described in section \ref{sec:experiment-methodology-parameter-tuning}, we were able to corroborate the space and time complexity of the procedure and understand better it's embedding mechanism. We chose the swiss roll and MNIST datasets for the tests because of their popularity, which makes it easier to compare our method with others in the literature, and because of how well Isomap and t-SNE embedded them. That allowed us to go even further and succesfully unfold a $10^8$ points swiss roll with divide-and-conquer Isomap. On the other hand, the classification task in the MNIST dataset proved more complicated to our algorithm, which was slower and separated digits worse than \verb|openTSNE|'s implementation of t-SNE.

The experimentation and evaluation process also provided us some insights on the standard SMACOF, LMDS, Isomap and t-SNE techniques. For example, we realized that LMDS, a nonlinear method intended to overcome the limitations of the SMACOF algorithm \citep{Chen2009}, was not able to unfold the swiss roll dataset. Even after having tuned the $k$ and $\tau$ parameters (see Algorithm \ref{alg:LMDS}), its embedding was porous and irregular instead of uniform and rectangular (see Figure \ref{fig:LMDS-swiss-roll}). Further research on this problem might lead to the development of a variation of LMDS that reduces the dimensionality of the swiss roll better.

Finally, we will describe the computer system we perfomed the tests on. Even though our main development computer was a  Macbook Pro (14-inc, Nov 2023) with 16 GB of RAM and the Apple M3 chip, we noticed that the \verb|concurrent.futures| module, which parallelized the execution of Algorithm \ref{alg:DivideConquer}, did not work in Mac computers with ARM chipsets. Therefore, we ended up using a Windows system. Specifically, our PC was an Asus ROG G513QM-HF026 laptop with the AMD Ryzen 7 5800H CPU, 16 GB of DDR4-3200MHz RAM, an SSD M.2 NVMe PCIe 3.0 and an NVIDIA RTX 3060 GPU.

\subsection{Initial runtime benchmarks}

Isomap was the first DR method we implemented into our divide-and-conquer framework, so it also was the first method we benchmarked. We tested divide-and-conquer Isomap on the swiss roll dataset with different parameter combinations and with parallel and serial computation. However, in all tests the number of connecting points for the Procrustes transformation was 100. We chose this number because, based on the thesis directors' experience on big data MDS \citep{Delicado2024}, it guaranteed good links between partitions' embeddings and efficient computations when $1,000 \leq l \leq 10,000$. Figure \ref{fig:Isomap-benchmark} shows the average runtimes of 20 experiments with different sets of parameters and dataset sizes. Parameters were previously tuned to ensure embeddings would preserve the structure of the data. As described in section \ref{sec:experiment-methodology-parameter-tuning}, we applied bare Isomap to swiss rolls of 1,000, 3,162 and 10,000 points, following a logarithmic sequence. Notice that we increased the number of neighbors $k$ for larger values of $l$ because partitions were denser.

After some experimentatiton, we realized that if $l=3162$ and $k=10$, the embedding was nearly perfect no matter the amount of individuals (see Figure \ref{fig:Isomap-huge}). Meanwhile, when $l=10,000$ and $k=15$, quality was similar and time was significantly larger, so we used $l=3162$ and $k=10$ for the largest datasets. This way, we managed to embed $10^8$ three-dimensional points into the Euclidean plane in about 3 hours. Hence, we showed that divide-and-conquer Isomap is capable of handling arbitrarily large datasets on a standard computer while maintaining the quality of the embedding.

Regarding parallelization, we can observe in Figure \ref{fig:Isomap-benchmark} that it effectively reduces the time complexity of divide-and-conquer DR, although its overhead slows down the algorithm when $n \leq 10^4$. Overall, results show that divide-and-conquer Isomap is linear in time with respect to $n$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Isomap-benchmark.png}
    \caption{Runtime (s) of divide-and-conquer Isomap averaged over 20 experiments. Tests were performed on datasets generated on the swiss roll manifold \citep{Spiwokv2007} with sizes ranging from $10^3$ to $10^8$. Data was embedded into $\mathbb{R}^2$ with different parameter combinations and $c=100$. Runtime in parallel and serial execution is also compared.}
    \label{fig:Isomap-benchmark}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Isomap-huge.png}
    \caption{Bidimensional embedding of a $10^8$ points swiss roll dataset \citep{Spiwokv2007} computed by divide-and-conquer Isomap with $l=3,162, \, c=100$ and $k=10$. Color represents the angle of rotation along the swiss roll spiral.}
    \label{fig:Isomap-huge}
\end{figure}

Afterwards, we tested divide-and-conquer t-SNE on the same datasets (see Figure \ref{fig:t-SNE-benchmark}). However, t-SNE performed notably slower than Isomap on the swiss roll, so we only run its divide-and-conquer variation with one parameter combination, $c = 100, \, l=1,000, \, Perp=30, \, n\_iter=250$. $n\_iter$ is the number of iterations carried out to minimize the Kullback-Leibler divergence \citep{Kullback1951}.

Even though divide-and-conquer t-SNE is about two orders of magnitud slower than divide-and-conquer Isomap, time complexity is linear as well, proving the expected results. The quality of the embedding, on the other hand, is very low. See Figure \ref{fig:t-SNE-huge} to observe that the structure of the data is broken into separate parts and the spiral shape is not unfolded.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/tSNE-benchmark.png}
    \caption{Runtime (s) of divide-and-conquer Isomap and divide-and-conquer t-SNE averaged over 20 experiments. Tests were performed on datasets generated on the swiss roll manifold \citep{Spiwokv2007} with sizes ranging from $10^3$ to $10^8$. Data was embedded into $\mathbb{R}^2$ with different parameter combinations and $c=100$.}
    \label{fig:t-SNE-benchmark}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/t-SNE-swiss-roll-huge.png}
    \caption{Bidimensional embedding of a $10^6$ points swiss roll dataset \citep{Spiwokv2007} computed by divide-and-conquer t-SNE with $l=1,000, \, c=100, \, Perp=30$ and $n\_iter=250$. Color represents the angle of rotation along the swiss roll spiral.}
    \label{fig:t-SNE-huge}
\end{figure}

\subsection{Analysis of possible overheads in divide-and-conquer DR}

One reasonable question to formulate about the divide-and-conquer approach is whether splitting the data and merging the resulting embeddings constitue a significant computational cost in comparison to reducing each partition's dimensionality. Theoretically, fractionating the data should be rapid. As for the merging of partitions' embeddings, in section \ref{sec:specification-and-design-of-the-solution} we depicted how we align and overlap them with Procustes transformations between each embedding and one in specific (the first one). We intentionally find a rigid transformation only with a random subset of $c < l$ points to accelerate its computation, and then multiply the full partition's embedding with the $q\times q$ matrix of the transformation. Therefore, the overhead of merging embeddings should be insignificant.

Table \ref{tab:dc-overhead} shows the results of an experiment where each part of the divide-and-conquer DR algorithm was independently timed. We uniformly sampled 5,000 points from the MNIST dataset and embedded them into the Euclidean plane with divide-and-conquer SMACOF. The arguments used were $l=1000,\, c=100,\, n\_iter = 300,\, \varepsilon = 0.001$. As it was expected, neither partitioning the dataset nor aligning the partial embeddings entail a noteworthy overhead in divide-and-conquer DR. Indeed, the prior and the latter were about 5,506 times and 50,710 times swifter than embedding all partitions with SMACOF, respectively.

\begin{table}[ht]
    \centering
    \begin{tabular}{lccc}
        \toprule
        Operation    & Divide & Embed & Merge \\
        \midrule
        Duration (s) & $6.88 \times 10^{-3}$ & 37.89 & $7.47 \times 10^{-4}$ \\
        \bottomrule
    \end{tabular}
    \caption{Runtime (s) of each step of divide-and-conquer SMACOF on a 5000-point random subset of MNIST. The arguments used were $l=1000,\, c=100,\, n\_iter = 300,\, \varepsilon = 0.001$.}
    \label{tab:dc-overhead}
\end{table}

\subsection{Divide-and-conquer SMACOF}

\subsubsection{Swiss roll}

In this experiment, we applied bare and divide-and-conquer SMACOF to swiss rolls of different sizes. The largest dataset the standard method could handle ended up having 7,500 individuals and taking about 16 minutes to calculate. When we applied bare SMACOF to a 10,000 points swiss roll, the system crashed because it lacked main memory. Therefore, we show in Figure \ref{fig:SMACOF-swiss-roll-7500} the results of the test with 7,500 points.

The spiraled shape of both embeddings suggest that SMACOF cannot identify the intrinsic bidimensionality of the swiss roll. Moreover, SMACOF condenses more the outer part of the manifold than the inner one and presents rugged edges. Regarding the qualitative differences between bare and divide-and-conquer versions, there is not much to be said. Divide-and-conquer SMACOF makes a good job aligning the partial configurations and obtains a similar embedding than bare SMACOF up to a 180º rotation.

To conclude, even though the SMACOF algorithm is not capable of properly embedding the swiss roll, we can see a clear advantage in using it with the divide-and-conquer framework on other datasets. Indeed, divide-and-conquer SMACOF is about 22 times faster than the bare method while returning a similar low-dimensional configuration.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/SMACOF-swiss-roll-7500.png}
    \caption{Comparison of the bidimensional embeddings of a 7,500 points swiss roll dataset \citep{Spiwokv2007} by bare (left) and divide-and-conquer (right) SMACOF. The arguments used were $n\_iter = 300,\, \varepsilon = 0.001$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Color represents the angle of rotation along the swiss roll spiral.}
    \label{fig:SMACOF-swiss-roll-7500}
\end{figure}


\subsubsection{MNIST}



Next, we try with a subpartition of 5000 images, since that worked properly with the swiss roll. We tried a few parameter combinations and obtained the best one for ... Figure \ref{fig:SMACOF-MNIST-kde} shows it is not bad, although 4, 7 and 9 are expectedly mixed. 5 could also be confused by 2 and 8 and viceversa. All normal here, their shapes are indeed similar.

Maybe I should talk about dimension correlation between SMACOF and divide-and-conquer SMACOF. The point of this is the framework, not the DR method, you know?

Total correlation of dim1: -0.8996107183598692
Total correlation of dim2: 0.8785425518794896

Merda, i falta el temps.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/SMACOF-MNIST-kde.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST \citep{Cohen2017} by bare (left) and divide-and-conquer (right) SMACOF. The arguments we used were $n\_iter = 300,\, \varepsilon = 0.001$ and in divide-and-conquer there also were $l=1000$ and $c=100$. We applied a Procrustes transformation to align both configurations. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:SMACOF-MNIST-kde}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/SMACOF-MNIST-coords.png}
    \caption{Scatter plots of first (left) and second (right) coordinates in the embeddings represented in Figure \ref{fig:SMACOF-MNIST-kde}. SMACOF is compared against divide-and-conquer SMACOF.}
    \label{fig:SMACOF-MNIST-coords}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/SMACOF-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the MNIST dataset \citep{Cohen2017} by divide-and-conquer SMACOF. The arguments used were $n\_iter = 300,\, \varepsilon = 0.001, \, l=1000$ and $c=100$. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:SMACOF-MNIST-huge}
\end{figure}

\subsection{Divide-and-conquer LMDS}

\subsubsection{Swiss roll}

Consider a swiss roll of 1,000 points. Well, LMDS makes trash out of it. Figure \ref{fig:LMDS-swiss-roll}.

\textit{POSSIBLE ANNEX EXPLICANT EL QUE HEM PROVAT PER MILLORAR LMDS (LMDS\_examples.ipynb)}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/LMDS-swiss-roll.png}
    \caption{Bidimensional embedding of a 1,000 points swiss roll dataset \citep{Spiwokv2007} computed by LMDS with $k=10$ and $\tau = 0.1$. Color represents the angle of rotation along the swiss roll spiral.}
    \label{fig:LMDS-swiss-roll}
\end{figure}

\subsubsection{MNIST}

Same 5000 points MNIST subset, mate. Oh, but this is very bad... Like, worse than SMACOF HAHAHAHAHA.

Parla del temps i la correlació dimensionaaaaaaal.

Total correlation of dim1: 0.9489962757561992
Total correlation of dim2: -0.8206029210441554

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/LMDS-MNIST.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST \citep{Cohen2017} by bare (left) and divide-and-conquer (right) LMDS. The arguments used were $k=10,\, \tau = 1$ and in divide-and-conquer there also were $l=1000$ and $c=100$. We applied a Procrustes transformation to align both configurations. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:LMDS-MNIST}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/LMDS-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the MNIST dataset \citep{Cohen2017} by divide-and-conquer LMDS. The arguments used were $k=10,\, \tau = 1, \, l=1000$ and $c=100$. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:LMDS-MNIST-huge}
\end{figure}

\subsection{Divide-and-conquer Isomap}

\subsubsection{Swiss roll}

Aaaaah... I don't have the same here. Gotta get those damn plots. Well we've seen this before, in the initial benchmark. Just go see Figure \ref{fig:Isomap-huge}.

\subsubsection{MNIST}

There you go, Figure \ref{fig:Isomap-MNIST}. It's just so bad HAHAHAHAHAH I can't handle this anymore.

Total correlation of dim1: 0.9195527127381181
Total correlation of dim2: -0.8623571714329816

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/Isomap-MNIST.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST \citep{Cohen2017} by bare (left) and divide-and-conquer (right) Isomap. The arguments used were $k=5$ and in divide-and-conquer there also were $l=1000$ and $c=100$. We applied a Procrustes transformation to align both configurations. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:Isomap-MNIST}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/Isomap-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the MNIST dataset \citep{Cohen2017} by divide-and-conquer Isomap. The arguments used were $k=5, \, l=1000$ and $c=100$. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:Isomap-MNIST-huge}
\end{figure}

\subsection{Divide-and-conquer t-SNE}

\subsubsection{Swiss roll}

t-SNE is specially useful for visualization, so we will focus on MNIST. Nonetheless, here's the swiss roll. Figure \ref{fig:t-SNE-huge}.

\subsubsection{MNIST}

EXPLICAR EXEMPLE DUES PARTICIONS: LA QUALITAT POT SER DOLENTA PERQUÈ ELS CLUSTERS TENEN UNA TOPOLOGIA DIFERENT

Total correlation of dim1: 0.8170763484715383
Total correlation of dim2: 0.32817864544487557

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/t-SNE-MNIST-partitions.png}
    \caption{Kernel density estimation of the bidimensional embeddings of two halves of the MNIST dataset \citep{Cohen2017}. Data was randomly ordered before being splitted. The DR method used was divide-and-conquer t-SNE with $Perp=30$. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:t-SNE-MNIST-partitions}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/t-SNE-MNIST-5000.png}
    \caption{Kernel density estimation of the bidimensional embeddings of a 5,000 points subset of MNIST \citep{Cohen2017} by bare (left) and divide-and-conquer (right) t-SNE. The arguments used were $Perp=30$ and in divide-and-conquer there also were $l=1000$ and $c=100$. We applied a Procrustes transformation to align both configurations. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:t-SNE-MNIST}
\end{figure}

AAAAAAAAra bé, l'original no només és millor; també és més ràpid! Va, mira què passa quan busquem una combinació de paràmetres pel t-SNE que funcioni bé amb 1000 dibuixets de números. Figure \ref{fig:t-SNE-MNIST-huge}.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/t-SNE-MNIST-huge.png}
    \caption{Kernel density estimation of the bidimensional embeddings of the MNIST dataset \citep{Cohen2017} by bare (left) and divide-and-conquer (right) t-SNE. The arguments used were $Perp=20, \, n\_iter=100$ and in divide-and-conquer there also were $l=1000$ and $c=100$. Countour lines are at 70\% of the maximum estimated density for each digit and embedding.}
    \label{fig:t-SNE-MNIST-huge}
\end{figure}


Conclusió: no hem guanyat aquesta batalla perquè la competència és un F1 i el nostre és un turisme. Però oi que no t'emportaries un F1 a la muntanya? Aaaaah-mic!

I ja estaria, xd. Anem bé de pàgines, béeeeeeeee.