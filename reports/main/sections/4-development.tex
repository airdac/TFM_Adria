\section{Development of the proposal}

\subsection{Python implementation of divide-and-conquer DR}

Given that R and Python are the standard programming languages in the data science field, we chose them as appropriate to implement the divide-and-conquer  DR algorithm. Initially, we aimed to develop and publish an R library because the thesis directors already had experience with the language. However, after reviewing the literature on DR for big data, we realized that many solutions were implemented in Python instead \citep{Reichmann2024}. So, in order to leverage the existing coding ecosystem, we switched to Python. From that moment onward, we documented the code development on an open-source GitHub repository (\href{https://github.com/airdac/TFM_Adria}{https://github.com/airdac/TFM\_Adria}). This system allowed us to easily update and share our implementations and experiments.

With time, the Python functions and classes we have written in different modules have become structured in a directory tree, hence taking the form of a package. Even though our project has not been published in any Python package index yet, it effectively works as a library. The main function is \verb|divide_conquer|, which implements Algorithm \ref{alg:DivideConquer} in parallel through the \verb|concurrent.futures| module. \verb|divide_conquer| also depends on private methods and requires a \verb|DRMethod| object as one of its arguments. This class, inherited from \verb|enum.Enum|, lists the supported DR methods in our package, which can be called through the \verb|get_method_function| method.

We have implemented and tested four DR algorithms: SMACOF, LMDS, Isomap and t-SNE. All but LMDS are wrappers to methods coded in other efficient and parallelized Python libraries. Specifically, we used the \verb|sklearn.manifold| module \citep{Pedregosa2011} for Isomap and SMACOF and \verb|openTSNE| \citep{Poliƒçar2023} for t-SNE. LMDS, on the other hand, is a less popular method and, up to our knowledge, it has no public Python implementation at the moment. However, the R library \verb|smacofx| \citep{Leeuw2009} does, so we translated it to Python with the help of the \verb|scipy.spatial.distance| and \verb|sklearn.neighbors| modules. This also allowed us to optimize the LMDS runtime with the \verb|numba| just-in-time compiler \citep{Lam2015, Aycock2003}.

As shown in Section \ref{sec:experimentation-and-evaluation}, our implementation does not add a significant computation overhead when compared to standard DR techniques. In fact, divide-and-conquer DR leverages parallelization and provides a significant speed up to standard DR methods. Most DR techniques have quadratic time complexity \citep{Kruskal1964a,Kruskal1964b,Chen2009, Vandermaaten2008}, except for Isomap, whose computation time is $\mathcal{O}(n^2\log(n))$ \citep{Tenenbaum2000}. Meanwhile, divide-and-conquer DR applies a DR method on $n/l$ partitions of $l$ individuals, resulting in linear time complexity: $\mathcal{O}(nl)$ usually and $\mathcal{O}(nl\log(l))$ for Isomap. The only DR method we have not been able to accelerate has been t-SNE, given that it already was thouroughly optimized in the \verb|openTSNE| library \citep{Policar2024}. Finally, since we only keep in memory the inter-individual distance matrix of the partitions being embedded at each moment, our DR framework reduces space complexity from $\mathcal{O}(n^2)$ to $\mathcal{O}(l^2)$.


\subsection{Experiments' methodology and parameter tuning}
\label{sec:experiment-methodology-parameter-tuning}

Once our framework was developed and we had implemented a few DR methods to test it, we run a series of experiments on the MNIST \citep{Cohen2017} and the Swiss roll \citep{Tenenbaum2000} datasets. We downloaded the MNIST dataset from the NIST webpage \citep{NIST2024} and generated points on the Swiss roll manifold with the \verb|sklearn.datasets.make_swiss_roll| function \citep{Pedregosa2011}. All these experiments were executed, logged, plotted and discussed on Jupyter notebooks \citep{Kluyver2016} with the help of the Python packages \verb|numpy| \citep{Harris2020}, \verb|pandas| \citep{Mckinney2010}, \verb|matplotlib.pyplot| \citep{Hunter2007}, \verb|os|, \verb|time|, \verb|sys|, \verb|shutil|, \verb|logging| and \verb|warnings|. Furthermore, results were serialized with the \verb|pickle| module in final tests.

The MNIST dataset consisted of 345,035 pictures of hand-drawn digits represented in $\mathbb{R}^{784}$. Each dimension would correspond to the intensity of a pixel in grayscale and there were $28 \times 28$ of them. See Figure \ref{fig:MNIST} to observe a few pictures in the dataset. On the other hand, the Swiss roll dataset was sampled from a bidimensional object in $\mathbb{R}^3$ (see Figure \ref{fig:swiss-roll}) shaped like a sheet of paper curved into a spiral along its longest axis. In order to measure the time complexity of divide-and-conquer DR, we randomly generated datasets with sizes ranging between $10^3$ and $10^8$ points. A good embedding of the Swiss roll into $\mathbb{R}^2$ should intuitively be a rectangle, as if the Swiss roll were unfolded. In MNIST, we would like to obtain 10 clearly distinct clusters corresponding to each digit present in the dataset.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/swiss-roll.png}
    \caption{The Swiss roll manifold. Color represents the angle of rotation along the spiral.}
    \label{fig:swiss-roll}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{figures/MNIST.png}
    \caption{Four sampled images of the MNIST dataset.}
    \label{fig:MNIST}
\end{figure}

The general experimental methodology we used consisted in a series of steps. Given that working with big datasets is costly, we randomly sampled a subset of 1000 points and embedded them into $\mathbb{R}^2$ with a traditional DR technique. To distinguish it from its divide-and-conquer version, we called it a \textit{bare} technique. Then, before increasing the size of the dataset, we tuned the parameters of the method by applying it to the same dataset with different value combinations. In the tuning process, we took into account the runtime and embedding quality for each value combination. Our goal was to find parameter values that provided a good trade-off between speed and preserving the structure of the data. In Section \ref{sec:experimentation-and-evaluation}, we enter into more detail on the qualitative assesment of each dataset's embeddings.

Once parameters were tuned with a small enough subset of the data for our system to handle, we applied divide-and-conquer DR to larger datasets with partitions of $l=1000$ points. In comparison, when we applied the bare techniques to big datasets, execution would crash because of a lack of main memory. Hence, as shown in Section \ref{sec:experimentation-and-evaluation}, we managed to extend the functionality of DR techniques to arbitrarily large datasets.