\section{Introduction, motivation, and objectives}

\subsection{Introduction}

Dimensionality reduction (DR) techniques embed high-dimensional datasets into signficantly lower-dimensional spaces while preserving the structure of the original data. Their primary goal is to tackle the common challenges of working with high-dimensional data, such as sparsity (caused by the curse of dimensionality) or large computational and storage costs. That being said, DR is mainly used for visualization purposes. In this setting, complex high-dimensional data is projected into $\mathbb{R}^2$, making patterns and clusters more apparent.

Many DR methods have been proposed since principal component analysis (PCA) (being the standard linear method) was first introduced \citep{Pearson1901}, each with distinct approaches and objectives. In particular, non-linear techniques have turned out to be very useful thanks to their ability to preserve complex relationships. Some examples are classical multidimensional scaling (classical MDS) \citep{Torgerson1952, Gower1966}, local multidimensional scaling (local MDS) \citep{Chen2009}, Isomap \citep{Tenenbaum2000}, t-distributed stochastic neighbor embedding (t-SNE) \citep{Vandermaaten2008}, uniform manifold approximation and projection (UMAP) \citep{McInnes2018a} and autoencoders \citep{Baldi1989, Kramer1991}. All these methods differ in how they define and maintain relationships between points, although most of them try to preserve global structure and local neighborhoods.

Despite their utility, DR methods face a few limitations. Many algorithms require computing and/or keeping in memory pairwise distances between all data points, resulting in quadratic time complexity and memory requirements. This becomes prohibitive for large datasets with millions of points. Parameter selection presents challenges too, since there is no general consensus on the best way to tune them nor how to measure the quality of an embedding with a validation set. The substantial time complexities of these algorithms makes $k$-fold cross-validation very costly as well. Finally, understanding which aspects of the data are preserved and which are distorted when reducing the dimensionality is crucial to correctly interpret the results.

\subsection{Motivation}

The recent advent of large data has led to new challenges and technologies to face them. When the number of observations of a dataset is very big, distance-based DR methods become computationally prohibitive because of their quadratic time and memory complexities. For example, working with a dataset of 100,000 individuals would require to keep a distance-matrix of 10 billion floating-point numbers in memory. If double precision is used, then the computer in use shall have at least 80 GB of RAM.

Recently, \citet{Delicado2024} studied existing and new modifications of classical MDS to tackle big datasets, demonstrating significant improvements in computational efficiency while maintaining embedding quality. Even though most of them could not be generalized to arbitrary DR techniques, two of them followed more common approaches: divide-and-conquer and recursion. These consist in partitioning the distance matrix into submatrices small enough for the system to keep in main memory. Then, classical MDS is applied to every partition independently and the resulting embeddings are aligned and merged with Procustes transformations. That being said, the recursive approach suffers from low-quality embeddings because it might divide the distance matrix into too small partitions. Therefore, we were interested in generalizing the divide-and-conquer proposal to DR methods beyond classical MDS.

\subsection{Objectives}

The primary goal of this thesis is to propose a divide-and-conquer framework for any generic distance-based dimensionality reduction method that effectively decreases the method's time and memory complexities. Specifically, we aim to:

\begin{enumerate}
    \item Review the literature to analyze the properties and applications of the most used DR techniques.
    \item Develop a generalized framework for distance-based DR methods that leverages the divide-and-conquer strategy and the Procrustes transformation to reduce time and memory complexities.
    \item Implement and parallelize the proposed framework for specific DR algorithms such as non-classical MDS, Local MDS, Isomap and t-SNE.
    \item Empirically evaluate the performance of the adapted algorithms in terms of computational efficiency, size limitations and quality of embeddings on benchmark datasets of varying sizes.
    \item Compare the results with the traditional counterpart of each tested DR method.
    \item Provide guidelines and best practices for selecting and tuning the proposed DR methods based on dataset characteristics and desired properties of the embedding.
\end{enumerate}

The successful completion of these objectives will contribute to making  dimensionality reduction techniques accessible for very large datasets, democratizing their use across scientific and industrial applications where data scale is a present challenge.

\subsection{Structure of this master thesis}

The chapters of this master's thesis are organized as follows:

\begin{itemize}
    \item \textbf{Chapter 2: State of the art} reviews four dimensionality reduction techniques (non-classical MDS, LMDS, Isomap, and t-SNE) key to this work. It also examines the work of \citet{Delicado2024} on classical MDS for big data to provide context for the chosen divide-and-conquer approach. Finally, it discusses alternative solutions for reducing the dimensionality of big datasets, such as landmark Isomap and the \verb|openTSNE| Python package.
    \item \textbf{Chapter 3: Specification and design of the solution} details the proposed divide-and-conquer framework. It explains how data is partitioned and how the resulting embeddings are merged with an efficient alignment procedure known as orthogonal Procrustes transformation.
    \item \textbf{Chapter 4: Development of the proposal} describes the implementation of the framework and utilized DR methods in Python, leveraging existing libraries like \verb|concurrent.futures| and \verb|sklearn.manifold|. Moreover, it outlines the methodology for the conducted experiments, including the datasets used (Swiss roll and MNIST) and the tuning of parameters.
    \item \textbf{Chapter 5: Experimentation and evaluation of the proposal} presents the results of the tests performed on divide-and-conquer DR. Specifically, it contains runtime benchmarks for Isomap and t-SNE, an analysis of computational overheads and a qualitative analysis of the embeddings produced by the divide-and-conquer versions of SMACOF, LMDS, Isomap, and t-SNE on the benchmark datasets.
    \item \textbf{Chapter 6: Analysis of sustainability and ethical implications} discusses the environmental benefits of the proposed algorithm in terms of reduced computational storage costs and energy consumption. Aditionally, it addresses how the method can improve the visibility of small communities within datasets by embedding more inidividuals.
    \item \textbf{Chapter 7: Conclusions} summarizes the key findings and outlines potential directions for future research. 
\end{itemize}